{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)]\n",
    "y = (iris.target == 0).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = per_clf.predict([[2,0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    return 1 if x >0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_perceptron(arr, bias, wts_1, wts_2):\n",
    "    x_11 = step(arr @ wts_1[0,:2].T + bias[0]*wts_1[0,-1])\n",
    "    x_12 = step(arr @ wts_1[1,:2].T + bias[0]*wts_1[1, -1])\n",
    "    \n",
    "    print(x_11, x_12)\n",
    "    hidden = np.array([x_11, x_12])\n",
    "    \n",
    "    \n",
    "    out = step( hidden @ wts_2[:2].T + bias[1]*wts_2[-1])\n",
    "    \n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[0,0], [1,1], [0,1], [1,0]])\n",
    "bias = np.array([1,1])\n",
    "wts_1 = np.array([[1,1,-1.5], [1,1,-0.5]])\n",
    "wts_2 = np.array([-1,1,-0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0\n",
      "1 1\n",
      "0\n",
      "0 1\n",
      "1\n",
      "0 1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(array)):\n",
    "    print(xor_perceptron(array[i], bias, wts_1, wts_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x28c69bfb7f0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x28c69c157f0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x28c69c2c880>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x28c71dfedc0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights,biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00638384, -0.00868036,  0.02692527, ...,  0.05918624,\n",
       "         0.03220224,  0.06377846],\n",
       "       [-0.04111947, -0.04777304,  0.01960474, ...,  0.06176805,\n",
       "        -0.01008502,  0.00022317],\n",
       "       [ 0.04639112,  0.0487331 , -0.0089742 , ...,  0.05517389,\n",
       "        -0.01149741, -0.06561173],\n",
       "       ...,\n",
       "       [-0.07077455,  0.00534277,  0.03196858, ...,  0.04168903,\n",
       "         0.06468014,  0.04547915],\n",
       "       [ 0.05100968,  0.0668398 ,  0.07123232, ..., -0.01434751,\n",
       "        -0.05607205, -0.0277362 ],\n",
       "       [-0.02192645, -0.0662646 ,  0.02927129, ...,  0.00240985,\n",
       "         0.01174528, -0.0340361 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 2s 989us/step - loss: 0.7292 - accuracy: 0.7596 - val_loss: 0.5346 - val_accuracy: 0.8168\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 2s 910us/step - loss: 0.4936 - accuracy: 0.8285 - val_loss: 0.4485 - val_accuracy: 0.8460\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 922us/step - loss: 0.4478 - accuracy: 0.8426 - val_loss: 0.4105 - val_accuracy: 0.8590\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 2s 945us/step - loss: 0.4199 - accuracy: 0.8520 - val_loss: 0.3993 - val_accuracy: 0.8650\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 2s 919us/step - loss: 0.4019 - accuracy: 0.8583 - val_loss: 0.4072 - val_accuracy: 0.8526\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 935us/step - loss: 0.3847 - accuracy: 0.8644 - val_loss: 0.4397 - val_accuracy: 0.8428\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 2s 917us/step - loss: 0.3723 - accuracy: 0.8683 - val_loss: 0.3708 - val_accuracy: 0.8692\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 2s 917us/step - loss: 0.3609 - accuracy: 0.8705 - val_loss: 0.3666 - val_accuracy: 0.8734\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 926us/step - loss: 0.3509 - accuracy: 0.8747 - val_loss: 0.3588 - val_accuracy: 0.8738\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 2s 931us/step - loss: 0.3415 - accuracy: 0.8783 - val_loss: 0.3491 - val_accuracy: 0.8774\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 2s 917us/step - loss: 0.3333 - accuracy: 0.8816 - val_loss: 0.3393 - val_accuracy: 0.8796\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 2s 926us/step - loss: 0.3242 - accuracy: 0.8843 - val_loss: 0.3330 - val_accuracy: 0.8792\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 2s 941us/step - loss: 0.3177 - accuracy: 0.8864 - val_loss: 0.3358 - val_accuracy: 0.8806\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 2s 942us/step - loss: 0.3091 - accuracy: 0.8896 - val_loss: 0.3539 - val_accuracy: 0.8728\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 2s 919us/step - loss: 0.3045 - accuracy: 0.8920 - val_loss: 0.3243 - val_accuracy: 0.8842\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 2s 924us/step - loss: 0.2970 - accuracy: 0.8930 - val_loss: 0.3246 - val_accuracy: 0.8834\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 933us/step - loss: 0.2919 - accuracy: 0.8955 - val_loss: 0.3173 - val_accuracy: 0.8848\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 928us/step - loss: 0.2842 - accuracy: 0.8976 - val_loss: 0.3158 - val_accuracy: 0.8884\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 2s 919us/step - loss: 0.2803 - accuracy: 0.8990 - val_loss: 0.3254 - val_accuracy: 0.8840\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 928us/step - loss: 0.2746 - accuracy: 0.9011 - val_loss: 0.3171 - val_accuracy: 0.8924\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 924us/step - loss: 0.2698 - accuracy: 0.9027 - val_loss: 0.3229 - val_accuracy: 0.8866\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 2s 915us/step - loss: 0.2653 - accuracy: 0.9044 - val_loss: 0.3050 - val_accuracy: 0.8894\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 2s 938us/step - loss: 0.2604 - accuracy: 0.9064 - val_loss: 0.3128 - val_accuracy: 0.8894\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 928us/step - loss: 0.2555 - accuracy: 0.9080 - val_loss: 0.3049 - val_accuracy: 0.8882\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 924us/step - loss: 0.2514 - accuracy: 0.9099 - val_loss: 0.3032 - val_accuracy: 0.8926\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 2s 919us/step - loss: 0.2463 - accuracy: 0.9108 - val_loss: 0.2960 - val_accuracy: 0.8906\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 914us/step - loss: 0.2426 - accuracy: 0.9123 - val_loss: 0.2954 - val_accuracy: 0.8894\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 2s 924us/step - loss: 0.2390 - accuracy: 0.9129 - val_loss: 0.2964 - val_accuracy: 0.8932\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 2s 938us/step - loss: 0.2345 - accuracy: 0.9163 - val_loss: 0.3068 - val_accuracy: 0.8866\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 2s 917us/step - loss: 0.2305 - accuracy: 0.9172 - val_loss: 0.2958 - val_accuracy: 0.8914\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(12,6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error is running mean during epoch, validation error @ end of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 54.0343 - accuracy: 0.8671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[54.03425598144531, 0.8671000003814697]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test/255\n",
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-37-81ace37e545f>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax((model.predict(X_new) > 0.5).astype('int32'), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a regression MLP using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7889 - val_loss: 0.5790\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.5450 - val_loss: 0.5084\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4760 - val_loss: 0.4848\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.4601 - val_loss: 0.4768\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4510 - val_loss: 0.4682\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4447 - val_loss: 0.4608\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4355 - val_loss: 0.4516\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4326 - val_loss: 0.4505\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4259 - val_loss: 0.4498\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.4221 - val_loss: 0.4379\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4204 - val_loss: 0.4385\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4178 - val_loss: 0.4312\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.4121 - val_loss: 0.4329\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4129 - val_loss: 0.4247\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4187 - val_loss: 0.4391\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4143 - val_loss: 0.4369\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.4100 - val_loss: 0.4316\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3959 - val_loss: 0.4125\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3938 - val_loss: 0.4312\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3942 - val_loss: 0.4248\n",
      "162/162 [==============================] - 0s 420us/step - loss: 0.3866\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test,y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0984373],\n",
       "       [1.9130317],\n",
       "       [1.3263563]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(12,6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building complex models using the Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 30)           270         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30)           930         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            39          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 826us/step - loss: 1.7705 - val_loss: 1.0203\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.8261 - val_loss: 0.7249\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.6860 - val_loss: 0.6836\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.6482 - val_loss: 0.6492\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.6195 - val_loss: 0.6238\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.5959 - val_loss: 0.6120\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.5769 - val_loss: 0.5881\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.5616 - val_loss: 0.5750\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.5487 - val_loss: 0.5660\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.5368 - val_loss: 0.5681\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.5293 - val_loss: 0.5515\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.5196 - val_loss: 0.5367\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.5134 - val_loss: 0.5322\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.5080 - val_loss: 0.5285\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.5015 - val_loss: 0.5415\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4975 - val_loss: 0.5361\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4929 - val_loss: 0.5133\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4901 - val_loss: 0.5123\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4856 - val_loss: 0.5063\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4826 - val_loss: 0.5025\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'mse', optimizer = keras.optimizers.SGD(lr = 1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data = (X_valid, y_valid))\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send different subsets of data to different paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 859us/step - loss: 2.1049 - val_loss: 0.9028\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.7615 - val_loss: 0.7233\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.6604 - val_loss: 0.6589\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.6123 - val_loss: 0.6220\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.5829 - val_loss: 0.5966\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.5620 - val_loss: 0.5794\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.5465 - val_loss: 0.5636\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.5351 - val_loss: 0.5537\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.5267 - val_loss: 0.5456\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.5191 - val_loss: 0.5398\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 629us/step - loss: 0.5127 - val_loss: 0.5324\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.5082 - val_loss: 0.5275\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.5040 - val_loss: 0.5238\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4997 - val_loss: 0.5197\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4960 - val_loss: 0.5160\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 651us/step - loss: 0.4927 - val_loss: 0.5174\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4901 - val_loss: 0.5094\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4871 - val_loss: 0.5080\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4843 - val_loss: 0.5051\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4817 - val_loss: 0.5039\n",
      "162/162 [==============================] - 0s 476us/step - loss: 0.4724\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B, = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit([X_train_A, X_train_B], y_train, epochs=20, validation_data=([X_valid_A, X_valid_B], y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.1395 - main_output_loss: 1.9273 - aux_output_loss: 4.0489 - val_loss: 1.0648 - val_main_output_loss: 0.8398 - val_aux_output_loss: 3.0890\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 750us/step - loss: 0.9123 - main_output_loss: 0.7197 - aux_output_loss: 2.6456 - val_loss: 0.8383 - val_main_output_loss: 0.6960 - val_aux_output_loss: 2.1188\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.7696 - main_output_loss: 0.6350 - aux_output_loss: 1.9812 - val_loss: 0.7581 - val_main_output_loss: 0.6539 - val_aux_output_loss: 1.6960\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.7073 - main_output_loss: 0.5998 - aux_output_loss: 1.6749 - val_loss: 0.7046 - val_main_output_loss: 0.6154 - val_aux_output_loss: 1.5072\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.6719 - main_output_loss: 0.5775 - aux_output_loss: 1.5220 - val_loss: 0.6747 - val_main_output_loss: 0.5948 - val_aux_output_loss: 1.3947\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.6469 - main_output_loss: 0.5608 - aux_output_loss: 1.4219 - val_loss: 0.6523 - val_main_output_loss: 0.5778 - val_aux_output_loss: 1.3222\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.6280 - main_output_loss: 0.5473 - aux_output_loss: 1.3539 - val_loss: 0.6381 - val_main_output_loss: 0.5687 - val_aux_output_loss: 1.2631\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.6128 - main_output_loss: 0.5367 - aux_output_loss: 1.2975 - val_loss: 0.6273 - val_main_output_loss: 0.5618 - val_aux_output_loss: 1.2166\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.5987 - main_output_loss: 0.5263 - aux_output_loss: 1.2507 - val_loss: 0.6095 - val_main_output_loss: 0.5452 - val_aux_output_loss: 1.1891\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.5877 - main_output_loss: 0.5181 - aux_output_loss: 1.2143 - val_loss: 0.5989 - val_main_output_loss: 0.5368 - val_aux_output_loss: 1.1586\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.5782 - main_output_loss: 0.5111 - aux_output_loss: 1.1817 - val_loss: 0.5982 - val_main_output_loss: 0.5392 - val_aux_output_loss: 1.1294\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.5712 - main_output_loss: 0.5066 - aux_output_loss: 1.1531 - val_loss: 0.5895 - val_main_output_loss: 0.5320 - val_aux_output_loss: 1.1068\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.5650 - main_output_loss: 0.5025 - aux_output_loss: 1.1270 - val_loss: 0.5802 - val_main_output_loss: 0.5238 - val_aux_output_loss: 1.0875\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.5575 - main_output_loss: 0.4967 - aux_output_loss: 1.1041 - val_loss: 0.5717 - val_main_output_loss: 0.5162 - val_aux_output_loss: 1.0712\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.5525 - main_output_loss: 0.4935 - aux_output_loss: 1.0836 - val_loss: 0.5673 - val_main_output_loss: 0.5133 - val_aux_output_loss: 1.0531\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.5464 - main_output_loss: 0.4887 - aux_output_loss: 1.0651 - val_loss: 0.5615 - val_main_output_loss: 0.5086 - val_aux_output_loss: 1.0376\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.5436 - main_output_loss: 0.4877 - aux_output_loss: 1.0469 - val_loss: 0.5573 - val_main_output_loss: 0.5057 - val_aux_output_loss: 1.0220\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.5383 - main_output_loss: 0.4835 - aux_output_loss: 1.0317 - val_loss: 0.5610 - val_main_output_loss: 0.5118 - val_aux_output_loss: 1.0046\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.5338 - main_output_loss: 0.4804 - aux_output_loss: 1.0143 - val_loss: 0.5503 - val_main_output_loss: 0.5012 - val_aux_output_loss: 0.9922\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.5297 - main_output_loss: 0.4774 - aux_output_loss: 0.9998 - val_loss: 0.5454 - val_main_output_loss: 0.4971 - val_aux_output_loss: 0.9806\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=[\"mse\",\"mse\"], loss_weights = [0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B, = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit([X_train_A, X_train_B], [y_train,y_train], epochs=20, validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 543us/step - loss: 0.5229 - main_output_loss: 0.4705 - aux_output_loss: 0.9949\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028C68662DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Subclassing API to Build Dynamic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g. name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel(30, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.2622 - output_1_loss: 2.1470 - output_2_loss: 3.2996 - val_loss: 1.1865 - val_output_1_loss: 1.0664 - val_output_2_loss: 2.2667\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.9671 - output_1_loss: 0.8462 - output_2_loss: 2.0554 - val_loss: 0.8894 - val_output_1_loss: 0.7956 - val_output_2_loss: 1.7343\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.8230 - output_1_loss: 0.7215 - output_2_loss: 1.7367 - val_loss: 0.8084 - val_output_1_loss: 0.7269 - val_output_2_loss: 1.5424\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.7619 - output_1_loss: 0.6720 - output_2_loss: 1.5709 - val_loss: 0.7634 - val_output_1_loss: 0.6881 - val_output_2_loss: 1.4405\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.7234 - output_1_loss: 0.6403 - output_2_loss: 1.4713 - val_loss: 0.7308 - val_output_1_loss: 0.6598 - val_output_2_loss: 1.3702\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.6956 - output_1_loss: 0.6175 - output_2_loss: 1.3981 - val_loss: 0.7052 - val_output_1_loss: 0.6370 - val_output_2_loss: 1.3194\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.6740 - output_1_loss: 0.5997 - output_2_loss: 1.3427 - val_loss: 0.6871 - val_output_1_loss: 0.6213 - val_output_2_loss: 1.2795\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.6566 - output_1_loss: 0.5856 - output_2_loss: 1.2954 - val_loss: 0.6721 - val_output_1_loss: 0.6085 - val_output_2_loss: 1.2442\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.6416 - output_1_loss: 0.5735 - output_2_loss: 1.2549 - val_loss: 0.6562 - val_output_1_loss: 0.5942 - val_output_2_loss: 1.2145\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.6294 - output_1_loss: 0.5637 - output_2_loss: 1.2211 - val_loss: 0.6446 - val_output_1_loss: 0.5842 - val_output_2_loss: 1.1877\n",
      "162/162 [==============================] - 0s 543us/step - loss: 0.6181 - output_1_loss: 0.5530 - output_2_loss: 1.2036\n",
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028C721ECEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.229285 ]\n",
      " [1.4665778]\n",
      " [0.8609873]] \n",
      " [[1.8947594]\n",
      " [2.2547216]\n",
      " [1.4068943]] \n",
      " 0.6180992722511292 \n",
      " 0.5530473589897156 1.2035629749298096\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_main, '\\n', y_pred_aux, '\\n', total_loss, '\\n', main_loss, aux_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 804us/step - loss: 1.9026 - val_loss: 0.8032\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.6838 - val_loss: 0.6538\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.6114 - val_loss: 0.6131\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.5762 - val_loss: 0.5860\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.5504 - val_loss: 0.5641\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.5307 - val_loss: 0.5483\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.5155 - val_loss: 0.5352\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.5037 - val_loss: 0.5244\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4935 - val_loss: 0.5159\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.4855 - val_loss: 0.5078\n",
      "162/162 [==============================] - 0s 395us/step - loss: 0.4713\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028C9ED2C160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.31178  ],\n",
       "       [1.6251962],\n",
       "       [1.1134903]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x28c680b0760>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 837us/step - loss: 1.9026 - val_loss: 0.8032\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.6838 - val_loss: 0.6538\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.6114 - val_loss: 0.6131\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.5762 - val_loss: 0.5860\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.5504 - val_loss: 0.5641\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.5307 - val_loss: 0.5483\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.5155 - val_loss: 0.5352\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.5037 - val_loss: 0.5244\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.4935 - val_loss: 0.5159\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4855 - val_loss: 0.5078\n",
      "162/162 [==============================] - 0s 395us/step - loss: 0.4713\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4792 - val_loss: 0.5019\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4734 - val_loss: 0.4973\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4698 - val_loss: 0.4945\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4646 - val_loss: 0.4891\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4612 - val_loss: 0.4851\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4579 - val_loss: 0.4826\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4551 - val_loss: 0.4802\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4528 - val_loss: 0.4777\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4497 - val_loss: 0.4746\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.4471 - val_loss: 0.4715\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.4450 - val_loss: 0.4707\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4426 - val_loss: 0.4678\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4411 - val_loss: 0.4660\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4387 - val_loss: 0.4638\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4374 - val_loss: 0.4622\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4354 - val_loss: 0.4602\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4339 - val_loss: 0.4591\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4322 - val_loss: 0.4573\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4306 - val_loss: 0.4562\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4289 - val_loss: 0.4549\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4277 - val_loss: 0.4534\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4259 - val_loss: 0.4516\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4254 - val_loss: 0.4509\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4238 - val_loss: 0.4494\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4225 - val_loss: 0.4477\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4212 - val_loss: 0.4468\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4201 - val_loss: 0.4453\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4189 - val_loss: 0.4449\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.4179 - val_loss: 0.4445\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4166 - val_loss: 0.4423\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4159 - val_loss: 0.4412\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4146 - val_loss: 0.4402\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4138 - val_loss: 0.4392\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4122 - val_loss: 0.4379\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4118 - val_loss: 0.4379\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4103 - val_loss: 0.4365\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4093 - val_loss: 0.4364\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4085 - val_loss: 0.4341\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4074 - val_loss: 0.4346\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4053 - val_loss: 0.4384\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4055 - val_loss: 0.4317\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4040 - val_loss: 0.4306\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4033 - val_loss: 0.4291\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4023 - val_loss: 0.4288\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4010 - val_loss: 0.4273\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4003 - val_loss: 0.4265\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3992 - val_loss: 0.4257\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.3984 - val_loss: 0.4244\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.3972 - val_loss: 0.4238\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3965 - val_loss: 0.4228\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.3953 - val_loss: 0.4220\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.3951 - val_loss: 0.4216\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.3946 - val_loss: 0.4228\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3929 - val_loss: 0.4189\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 632us/step - loss: 0.3919 - val_loss: 0.4189\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 747us/step - loss: 0.3910 - val_loss: 0.4181\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.3899 - val_loss: 0.4161\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.3892 - val_loss: 0.4168\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.3886 - val_loss: 0.4146\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3873 - val_loss: 0.4147\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.3868 - val_loss: 0.4141\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 848us/step - loss: 0.3858 - val_loss: 0.4126\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 637us/step - loss: 0.3849 - val_loss: 0.4119\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.3845 - val_loss: 0.4106\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.3831 - val_loss: 0.4106\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3825 - val_loss: 0.4103\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3816 - val_loss: 0.4082\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.3808 - val_loss: 0.4069\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3801 - val_loss: 0.4076\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3797 - val_loss: 0.4062\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3784 - val_loss: 0.4051\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3776 - val_loss: 0.4052\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3770 - val_loss: 0.4046\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3776 - val_loss: 0.4040\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.3774 - val_loss: 0.4022\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.3745 - val_loss: 0.4027\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.3738 - val_loss: 0.4005\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.3735 - val_loss: 0.4007\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3734 - val_loss: 0.3998\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3715 - val_loss: 0.3991\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3710 - val_loss: 0.3980\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3703 - val_loss: 0.3983\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3699 - val_loss: 0.3972\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.3688 - val_loss: 0.3970\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3678 - val_loss: 0.3954\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3668 - val_loss: 0.3955\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3663 - val_loss: 0.3959\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3667 - val_loss: 0.3934\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.3652 - val_loss: 0.3932\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.3647 - val_loss: 0.3921\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3637 - val_loss: 0.3922\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3631 - val_loss: 0.3912\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.3621 - val_loss: 0.3907\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.3614 - val_loss: 0.3903\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.3610 - val_loss: 0.3887\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3626 - val_loss: 0.3893\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3633 - val_loss: 0.3888\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.3592 - val_loss: 0.3884\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.3590 - val_loss: 0.3883\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3596 - val_loss: 0.3865\n",
      "162/162 [==============================] - 0s 518us/step - loss: 0.3524\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343/363 [===========================>..] - ETA: 0s - loss: 0.3559\n",
      "val/train: 1.08\n",
      "363/363 [==============================] - 0s 893us/step - loss: 0.3572 - val_loss: 0.3853\n"
     ]
    }
   ],
   "source": [
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorboard for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2021_02_08-07_22_08'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/363 [..............................] - ETA: 0s - loss: 6.2460WARNING:tensorflow:From C:\\Users\\nbwal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/363 [..............................] - ETA: 2:43 - loss: 6.3954WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0040s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.9028s). Check your callbacks.\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.9026 - val_loss: 0.8032\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.6838 - val_loss: 0.6538\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.6114 - val_loss: 0.6131\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.5762 - val_loss: 0.5860\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.5504 - val_loss: 0.5641\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.5307 - val_loss: 0.5483\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 750us/step - loss: 0.5155 - val_loss: 0.5352\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 697us/step - loss: 0.5037 - val_loss: 0.5244\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4935 - val_loss: 0.5159\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.4855 - val_loss: 0.5078\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.4792 - val_loss: 0.5031\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.4735 - val_loss: 0.4975\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.4692 - val_loss: 0.4931\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4646 - val_loss: 0.4890\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 783us/step - loss: 0.4614 - val_loss: 0.4857\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4578 - val_loss: 0.4821\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4553 - val_loss: 0.4799\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4523 - val_loss: 0.4779\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4496 - val_loss: 0.4749\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4470 - val_loss: 0.4726\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.4449 - val_loss: 0.4711\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 662us/step - loss: 0.4423 - val_loss: 0.4678\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4411 - val_loss: 0.4670\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.4390 - val_loss: 0.4641\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.4371 - val_loss: 0.4620\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 673us/step - loss: 0.4352 - val_loss: 0.4601\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.4336 - val_loss: 0.4581\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 750us/step - loss: 0.4320 - val_loss: 0.4572\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 673us/step - loss: 0.4305 - val_loss: 0.4564\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 761us/step - loss: 0.4289 - val_loss: 0.4541\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18912), started 0:01:16 ago. (Use '!kill 18912' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-abafed17d29205d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-abafed17d29205d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=.\\\\my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1,1000+1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        data = (np.random.rand(100) + 2) * step / 100 # some random data\n",
    "        tf.summary.histogram('my_hist', data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3)  # random 32x32 RGB images\n",
    "        tf.summary.image('my_images', images * step / 1000, step=step)\n",
    "        texts = ['The step is ' + str(step), 'Its square is ' + str(step**2)]\n",
    "        tf.summary.text('my_text', texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio('my_audio', audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/363 [..............................] - ETA: 3:12 - loss: 6.3954WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 1.0633s). Check your callbacks.\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 1.9026 - val_loss: 0.8032\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.6838 - val_loss: 0.6538\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.6114 - val_loss: 0.6131\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.5762 - val_loss: 0.5860\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.5504 - val_loss: 0.5641\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.5307 - val_loss: 0.5483\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.5155 - val_loss: 0.5352\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.5037 - val_loss: 0.5244\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.4935 - val_loss: 0.5159\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4855 - val_loss: 0.5078\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4792 - val_loss: 0.5031\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4735 - val_loss: 0.4975\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4692 - val_loss: 0.4931\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.4646 - val_loss: 0.4890\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.4614 - val_loss: 0.4857\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.4578 - val_loss: 0.4821\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.4553 - val_loss: 0.4799\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.4523 - val_loss: 0.4779\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.4496 - val_loss: 0.4749\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 640us/step - loss: 0.4470 - val_loss: 0.4726\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4449 - val_loss: 0.4711\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4423 - val_loss: 0.4678\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4411 - val_loss: 0.4670\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.4390 - val_loss: 0.4641\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4371 - val_loss: 0.4620\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4352 - val_loss: 0.4601\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.4336 - val_loss: 0.4581\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.4320 - val_loss: 0.4572\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.4305 - val_loss: 0.4564\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.4289 - val_loss: 0.4541\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(test_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18912), started 0:15:58 ago. (Use '!kill 18912' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e7493afa1d44fd7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e7493afa1d44fd7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=.\\\\my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.1991 - val_loss: 0.7120\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.6445 - val_loss: 0.6145\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.5720 - val_loss: 0.6271\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.5334 - val_loss: 0.5358\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.5092 - val_loss: 0.5165\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.4952 - val_loss: 0.5026\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.4842 - val_loss: 0.4961\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4764 - val_loss: 0.4956\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4651 - val_loss: 0.4823\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4583 - val_loss: 0.4748\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4521 - val_loss: 0.4771\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4499 - val_loss: 0.4700\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4489 - val_loss: 0.4683\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4431 - val_loss: 0.4640\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4395 - val_loss: 0.4596\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.4372 - val_loss: 0.4594\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.4394 - val_loss: 0.4567\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4316 - val_loss: 0.4597\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.4294 - val_loss: 0.4549\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4268 - val_loss: 0.4494\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.4249 - val_loss: 0.4516\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4216 - val_loss: 0.4445\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.4200 - val_loss: 0.4424\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4187 - val_loss: 0.4407\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4165 - val_loss: 0.4414\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4136 - val_loss: 0.4374\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4126 - val_loss: 0.4343\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.4107 - val_loss: 0.4331\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4087 - val_loss: 0.4314\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4074 - val_loss: 0.4296\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4054 - val_loss: 0.4275\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4032 - val_loss: 0.4283\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.4017 - val_loss: 0.4305\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.4022 - val_loss: 0.4231\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3993 - val_loss: 0.4256\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3977 - val_loss: 0.4203\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3963 - val_loss: 0.4208\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3956 - val_loss: 0.4190\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3938 - val_loss: 0.4525\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3924 - val_loss: 0.4303\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3984 - val_loss: 0.4149\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3916 - val_loss: 0.4165\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3901 - val_loss: 0.4132\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3868 - val_loss: 0.4127\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3882 - val_loss: 0.4084\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3891 - val_loss: 0.4074\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3837 - val_loss: 0.4087\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3839 - val_loss: 0.4066\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3808 - val_loss: 0.4052\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3807 - val_loss: 0.4047\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3787 - val_loss: 0.4061\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3783 - val_loss: 0.4039\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3780 - val_loss: 0.4104\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3767 - val_loss: 0.4053\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3760 - val_loss: 0.4013\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3775 - val_loss: 0.4006\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3748 - val_loss: 0.3966\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3725 - val_loss: 0.3975\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3723 - val_loss: 0.4010\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3711 - val_loss: 0.3992\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3693 - val_loss: 0.3949\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3700 - val_loss: 0.3960\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3699 - val_loss: 0.3965\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3694 - val_loss: 0.3911\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3670 - val_loss: 0.4108\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3679 - val_loss: 0.3915\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3648 - val_loss: 0.3910\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.3694 - val_loss: 0.3864\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3666 - val_loss: 0.3896\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3626 - val_loss: 0.3862\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3658 - val_loss: 0.3885\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3605 - val_loss: 0.3858\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.3592 - val_loss: 0.3883\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3611 - val_loss: 0.4025\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3695 - val_loss: 0.3850\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3619 - val_loss: 0.3849\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3562 - val_loss: 0.3831\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3615 - val_loss: 0.3797\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3557 - val_loss: 0.3825\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3546 - val_loss: 0.3804\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3553 - val_loss: 0.3781\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3535 - val_loss: 0.3825\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3535 - val_loss: 0.3831\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3529 - val_loss: 0.3864\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3571 - val_loss: 0.3763\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3504 - val_loss: 0.3798\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3503 - val_loss: 0.3868\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3500 - val_loss: 0.3767\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.3524 - val_loss: 0.3775\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3567 - val_loss: 0.3755\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3491 - val_loss: 0.3814\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3494 - val_loss: 0.3736\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3486 - val_loss: 0.3731\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3488 - val_loss: 0.3744\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3465 - val_loss: 0.3722\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3495 - val_loss: 0.3715\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3467 - val_loss: 0.3736\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3454 - val_loss: 0.3718\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3443 - val_loss: 0.3738\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3444 - val_loss: 0.3718\n",
      "162/162 [==============================] - 0s 395us/step - loss: 0.3433\n",
      "WARNING:tensorflow:8 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028C7CCF35E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 3.5055 - val_loss: 1.9213\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 1.3107 - val_loss: 1.0237\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.8398 - val_loss: 0.8050\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.7169 - val_loss: 0.7340\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6732 - val_loss: 0.7062\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6506 - val_loss: 0.6829\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6344 - val_loss: 0.6693\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6211 - val_loss: 0.6550\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6100 - val_loss: 0.6491\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6005 - val_loss: 0.6401\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5920 - val_loss: 0.6306\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5844 - val_loss: 0.6223\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5779 - val_loss: 0.6153\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5714 - val_loss: 0.6169\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5667 - val_loss: 0.6122\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5624 - val_loss: 0.6068\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5582 - val_loss: 0.5984\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5549 - val_loss: 0.5968\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5512 - val_loss: 0.5989\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5489 - val_loss: 0.5919\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5458 - val_loss: 0.5936\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5429 - val_loss: 0.5817\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5418 - val_loss: 0.5794\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5398 - val_loss: 0.5867\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5367 - val_loss: 0.5742\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5369 - val_loss: 0.5730\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5353 - val_loss: 0.5704\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5353 - val_loss: 0.5734\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5328 - val_loss: 0.5688\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5320 - val_loss: 0.5659\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5322 - val_loss: 0.5692\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5310 - val_loss: 0.5680\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5304 - val_loss: 0.5788\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5297 - val_loss: 0.5683\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5293 - val_loss: 0.5681\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5279 - val_loss: 0.5624\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5276 - val_loss: 0.5745\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5276 - val_loss: 0.5643\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5272 - val_loss: 0.5702\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5270 - val_loss: 0.5730\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5266 - val_loss: 0.5712\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5262 - val_loss: 0.5627\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5262 - val_loss: 0.5623\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5254 - val_loss: 0.5579\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5265 - val_loss: 0.5635\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5241 - val_loss: 0.5563\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5253 - val_loss: 0.5695\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5251 - val_loss: 0.5620\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5251 - val_loss: 0.5593\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5246 - val_loss: 0.5689\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5250 - val_loss: 0.5691\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5236 - val_loss: 0.5563\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5246 - val_loss: 0.5671\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5247 - val_loss: 0.5623\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5233 - val_loss: 0.5723\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5217 - val_loss: 0.5537\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5247 - val_loss: 0.5556\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5219 - val_loss: 0.5534\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5246 - val_loss: 0.5591\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5230 - val_loss: 0.5544\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5242 - val_loss: 0.5559\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5245 - val_loss: 0.5587\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5238 - val_loss: 0.5567\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5237 - val_loss: 0.5591\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5238 - val_loss: 0.5569\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.5240 - val_loss: 0.5601\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.5236 - val_loss: 0.5682\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5207 - val_loss: 0.5519\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5233 - val_loss: 0.5511\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5242 - val_loss: 0.5540\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5227 - val_loss: 0.5515\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5233 - val_loss: 0.5525\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5235 - val_loss: 0.5655\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5221 - val_loss: 0.5530\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5232 - val_loss: 0.5535\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 599us/step - loss: 0.5232 - val_loss: 0.5517\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5243 - val_loss: 0.5548\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5227 - val_loss: 0.5523\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5232 - val_loss: 0.5517\n",
      "121/121 [==============================] - 0s 462us/step - loss: 0.5267\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 3.4838 - val_loss: 1.9250\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 1.4240 - val_loss: 1.1063\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.9773 - val_loss: 0.8990\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.8450 - val_loss: 0.8256\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7853 - val_loss: 0.7850\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.7474 - val_loss: 0.7537\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.7177 - val_loss: 0.7274\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6926 - val_loss: 0.7056\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6718 - val_loss: 0.6865\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6541 - val_loss: 0.6698\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6387 - val_loss: 0.6576\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6261 - val_loss: 0.6442\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6145 - val_loss: 0.6335\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6045 - val_loss: 0.6242\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5958 - val_loss: 0.6167\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5884 - val_loss: 0.6092\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5819 - val_loss: 0.6023\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5762 - val_loss: 0.5973\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5709 - val_loss: 0.5943\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5668 - val_loss: 0.5892\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5627 - val_loss: 0.5875\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5592 - val_loss: 0.5794\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5565 - val_loss: 0.5761\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5541 - val_loss: 0.5771\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5509 - val_loss: 0.5707\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5497 - val_loss: 0.5687\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5478 - val_loss: 0.5660\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5471 - val_loss: 0.5670\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5447 - val_loss: 0.5633\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5439 - val_loss: 0.5616\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5429 - val_loss: 0.5605\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5428 - val_loss: 0.5617\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5410 - val_loss: 0.5691\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5408 - val_loss: 0.5612\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5403 - val_loss: 0.5603\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5390 - val_loss: 0.5562\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5390 - val_loss: 0.5646\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5385 - val_loss: 0.5578\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5381 - val_loss: 0.5626\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5378 - val_loss: 0.5646\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5378 - val_loss: 0.5629\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5369 - val_loss: 0.5548\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5377 - val_loss: 0.5562\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.5367 - val_loss: 0.5532\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5376 - val_loss: 0.5551\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5358 - val_loss: 0.5512\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5369 - val_loss: 0.5606\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5363 - val_loss: 0.5550\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5366 - val_loss: 0.5537\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5359 - val_loss: 0.5628\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5363 - val_loss: 0.5643\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5351 - val_loss: 0.5517\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5366 - val_loss: 0.5595\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.5363 - val_loss: 0.5588\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5354 - val_loss: 0.5649\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5336 - val_loss: 0.5506\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5361 - val_loss: 0.5511\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5344 - val_loss: 0.5499\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5362 - val_loss: 0.5536\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5347 - val_loss: 0.5503\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5362 - val_loss: 0.5521\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5361 - val_loss: 0.5534\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5359 - val_loss: 0.5523\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5360 - val_loss: 0.5552\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5358 - val_loss: 0.5539\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5359 - val_loss: 0.5546\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5350 - val_loss: 0.5636\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5325 - val_loss: 0.5486\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5357 - val_loss: 0.5489\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5362 - val_loss: 0.5506\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5344 - val_loss: 0.5481\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5357 - val_loss: 0.5494\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5360 - val_loss: 0.5592\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5341 - val_loss: 0.5498\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5354 - val_loss: 0.5494\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5355 - val_loss: 0.5488\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5359 - val_loss: 0.5504\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5356 - val_loss: 0.5502\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5356 - val_loss: 0.5495\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5356 - val_loss: 0.5521\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5351 - val_loss: 0.5602\n",
      "121/121 [==============================] - 0s 463us/step - loss: 0.5073\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 3.1015 - val_loss: 1.6553\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.9832 - val_loss: 0.7714\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6062 - val_loss: 0.6014\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5347 - val_loss: 0.5649\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5202 - val_loss: 0.5547\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5169 - val_loss: 0.5519\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5157 - val_loss: 0.5503\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5152 - val_loss: 0.5496\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5147 - val_loss: 0.5491\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5142 - val_loss: 0.5485\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5139 - val_loss: 0.5485\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5136 - val_loss: 0.5484\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5132 - val_loss: 0.5483\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5129 - val_loss: 0.5483\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5127 - val_loss: 0.5479\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5125 - val_loss: 0.5477\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5124 - val_loss: 0.5475\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5122 - val_loss: 0.5476\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5121 - val_loss: 0.5473\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5118 - val_loss: 0.5468\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5118 - val_loss: 0.5471\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5116 - val_loss: 0.5468\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5114 - val_loss: 0.5468\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5115 - val_loss: 0.5467\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5115 - val_loss: 0.5469\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5112 - val_loss: 0.5471\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5112 - val_loss: 0.5471\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5112 - val_loss: 0.5465\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5111 - val_loss: 0.5469\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5112 - val_loss: 0.5470\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5111 - val_loss: 0.5468\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5110 - val_loss: 0.5471\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5109 - val_loss: 0.5474\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5110 - val_loss: 0.5473\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5109 - val_loss: 0.5467\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5109 - val_loss: 0.5470\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5108 - val_loss: 0.5468\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5109 - val_loss: 0.5471\n",
      "121/121 [==============================] - 0s 430us/step - loss: 0.5521\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 1.2622 - val_loss: 0.6615\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6267 - val_loss: 0.6092\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6089 - val_loss: 0.5913\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5888 - val_loss: 0.5643\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5558 - val_loss: 0.5624\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5335 - val_loss: 0.5522\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5393 - val_loss: 0.5580\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5380 - val_loss: 0.5505\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5639 - val_loss: 0.7773\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5510 - val_loss: 0.6704\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5557 - val_loss: 0.5482\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5415 - val_loss: 0.5464\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6067 - val_loss: 0.5483\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5298 - val_loss: 0.8170\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5516 - val_loss: 0.6317\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5676 - val_loss: 0.7036\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.7095 - val_loss: 0.5487\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5512 - val_loss: 0.5705\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5338 - val_loss: 0.7889\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5985 - val_loss: 0.5585\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5424 - val_loss: 0.7316\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5272 - val_loss: 0.5457\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5606 - val_loss: 0.5492\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5913 - val_loss: 0.6387\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5277 - val_loss: 0.5446\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5485 - val_loss: 0.5461\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5420 - val_loss: 0.5424\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6768 - val_loss: 0.5525\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5291 - val_loss: 0.5414\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6301 - val_loss: 0.5454\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5555 - val_loss: 0.5496\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5474 - val_loss: 0.5469\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5329 - val_loss: 1.0826\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5952 - val_loss: 0.5481\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5479 - val_loss: 0.5513\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5488 - val_loss: 0.5411\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5453 - val_loss: 0.6428\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.7507 - val_loss: 0.5448\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5431 - val_loss: 0.5863\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5436 - val_loss: 0.6144\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5385 - val_loss: 0.5775\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5795 - val_loss: 0.5505\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5395 - val_loss: 0.5444\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5535 - val_loss: 0.5403\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6699 - val_loss: 0.5559\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5315 - val_loss: 0.5429\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5564 - val_loss: 0.7196\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5428 - val_loss: 0.5473\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5568 - val_loss: 0.5433\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6593 - val_loss: 0.5922\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5628 - val_loss: 0.6907\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5361 - val_loss: 0.5443\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5501 - val_loss: 0.6411\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5585 - val_loss: 0.5566\n",
      "121/121 [==============================] - 0s 429us/step - loss: 0.5299\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 1.2501 - val_loss: 0.7182\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5960 - val_loss: 0.6114\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5798 - val_loss: 0.5803\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5667 - val_loss: 0.5600\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5900 - val_loss: 0.5604\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5495 - val_loss: 0.5514\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5525 - val_loss: 0.5666\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5496 - val_loss: 0.5476\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5627 - val_loss: 0.7072\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5596 - val_loss: 0.7349\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5593 - val_loss: 0.5494\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5661 - val_loss: 0.5484\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5615 - val_loss: 0.5525\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5516 - val_loss: 0.6515\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5499 - val_loss: 0.5943\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5584 - val_loss: 0.7202\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5537 - val_loss: 0.5504\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5636 - val_loss: 0.6215\n",
      "121/121 [==============================] - 0s 430us/step - loss: 0.5310\n",
      "Epoch 1/100\n",
      "237/242 [============================>.] - ETA: 0s - loss: 1.5045WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0040s). Check your callbacks.\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4822 - val_loss: 0.5455\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 4.8664 - val_loss: 0.9567\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 55.8622 - val_loss: 3.9519\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 839.7958 - val_loss: 50.5318\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 13482.5723 - val_loss: 589.2447\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 197177.5625 - val_loss: 9031.7744\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 2932060.0000 - val_loss: 131776.1875\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 41615476.0000 - val_loss: 2003469.7500\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 570973056.0000 - val_loss: 48880180.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 9264425984.0000 - val_loss: 430027168.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 127143968768.0000 - val_loss: 8790955008.0000\n",
      "121/121 [==============================] - 0s 430us/step - loss: 1043946733568.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6085 - val_loss: 1.4244\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 1.0911 - val_loss: 0.9222\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.8192 - val_loss: 0.8028\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.7406 - val_loss: 0.7516\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.7040 - val_loss: 0.7216\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6778 - val_loss: 0.6979\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6568 - val_loss: 0.6778\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6386 - val_loss: 0.6612\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6225 - val_loss: 0.6466\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6080 - val_loss: 0.6328\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5947 - val_loss: 0.6210\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5826 - val_loss: 0.6098\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5717 - val_loss: 0.5997\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5614 - val_loss: 0.5902\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5522 - val_loss: 0.5820\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5437 - val_loss: 0.5742\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.5357 - val_loss: 0.5673\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5286 - val_loss: 0.5602\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5219 - val_loss: 0.5541\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5159 - val_loss: 0.5485\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5101 - val_loss: 0.5432\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5046 - val_loss: 0.5380\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4999 - val_loss: 0.5333\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4952 - val_loss: 0.5294\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4908 - val_loss: 0.5248\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4869 - val_loss: 0.5211\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4830 - val_loss: 0.5175\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4796 - val_loss: 0.5137\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4760 - val_loss: 0.5105\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4731 - val_loss: 0.5075\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4701 - val_loss: 0.5046\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4673 - val_loss: 0.5016\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4647 - val_loss: 0.4993\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4621 - val_loss: 0.4964\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4597 - val_loss: 0.4940\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4574 - val_loss: 0.4913\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4549 - val_loss: 0.4895\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4530 - val_loss: 0.4870\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4508 - val_loss: 0.4849\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4488 - val_loss: 0.4829\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4470 - val_loss: 0.4809\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4452 - val_loss: 0.4784\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4433 - val_loss: 0.4767\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4418 - val_loss: 0.4748\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4399 - val_loss: 0.4731\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4384 - val_loss: 0.4713\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4370 - val_loss: 0.4694\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4354 - val_loss: 0.4678\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4339 - val_loss: 0.4666\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4326 - val_loss: 0.4645\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4313 - val_loss: 0.4631\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4299 - val_loss: 0.4619\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4286 - val_loss: 0.4605\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4272 - val_loss: 0.4593\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4261 - val_loss: 0.4577\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4249 - val_loss: 0.4562\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4238 - val_loss: 0.4551\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4226 - val_loss: 0.4538\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4215 - val_loss: 0.4527\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4205 - val_loss: 0.4514\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4191 - val_loss: 0.4508\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4183 - val_loss: 0.4492\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4173 - val_loss: 0.4484\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4164 - val_loss: 0.4471\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4153 - val_loss: 0.4460\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4143 - val_loss: 0.4457\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4136 - val_loss: 0.4441\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4125 - val_loss: 0.4430\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4116 - val_loss: 0.4423\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4109 - val_loss: 0.4415\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4100 - val_loss: 0.4403\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4091 - val_loss: 0.4395\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4084 - val_loss: 0.4385\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4075 - val_loss: 0.4377\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4067 - val_loss: 0.4370\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4059 - val_loss: 0.4365\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4051 - val_loss: 0.4357\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4044 - val_loss: 0.4343\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4038 - val_loss: 0.4338\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4029 - val_loss: 0.4330\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4020 - val_loss: 0.4326\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4016 - val_loss: 0.4313\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4005 - val_loss: 0.4320\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4002 - val_loss: 0.4300\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3993 - val_loss: 0.4294\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3988 - val_loss: 0.4284\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3981 - val_loss: 0.4277\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3974 - val_loss: 0.4273\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3967 - val_loss: 0.4263\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3961 - val_loss: 0.4258\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3954 - val_loss: 0.4254\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3949 - val_loss: 0.4247\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3941 - val_loss: 0.4241\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3935 - val_loss: 0.4235\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.3930 - val_loss: 0.4226\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3924 - val_loss: 0.4221\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3916 - val_loss: 0.4221\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3912 - val_loss: 0.4210\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3904 - val_loss: 0.4203\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3899 - val_loss: 0.4196\n",
      "121/121 [==============================] - 0s 496us/step - loss: 0.4054\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8990 - val_loss: 1.4753\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 1.1395 - val_loss: 0.9002\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.8215 - val_loss: 0.7688\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7266 - val_loss: 0.7136\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6827 - val_loss: 0.6848\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6541 - val_loss: 0.6608\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6319 - val_loss: 0.6419\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6136 - val_loss: 0.6261\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5983 - val_loss: 0.6131\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5852 - val_loss: 0.6014\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5739 - val_loss: 0.5912\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5640 - val_loss: 0.5818\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5551 - val_loss: 0.5740\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5474 - val_loss: 0.5662\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5402 - val_loss: 0.5599\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5339 - val_loss: 0.5533\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5281 - val_loss: 0.5482\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5228 - val_loss: 0.5424\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5178 - val_loss: 0.5379\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5134 - val_loss: 0.5335\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5090 - val_loss: 0.5293\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5049 - val_loss: 0.5253\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5012 - val_loss: 0.5223\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4978 - val_loss: 0.5188\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4945 - val_loss: 0.5146\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4912 - val_loss: 0.5119\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4881 - val_loss: 0.5086\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4854 - val_loss: 0.5059\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4825 - val_loss: 0.5032\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.4798 - val_loss: 0.5004\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4773 - val_loss: 0.4980\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4749 - val_loss: 0.4957\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4725 - val_loss: 0.4935\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4701 - val_loss: 0.4909\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4680 - val_loss: 0.4889\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4658 - val_loss: 0.4866\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4639 - val_loss: 0.4848\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4619 - val_loss: 0.4829\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4599 - val_loss: 0.4808\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4581 - val_loss: 0.4789\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4564 - val_loss: 0.4772\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4543 - val_loss: 0.4749\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4529 - val_loss: 0.4734\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4513 - val_loss: 0.4724\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4496 - val_loss: 0.4703\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4481 - val_loss: 0.4685\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4467 - val_loss: 0.4671\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4451 - val_loss: 0.4654\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4437 - val_loss: 0.4639\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4423 - val_loss: 0.4626\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4408 - val_loss: 0.4611\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4394 - val_loss: 0.4602\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4382 - val_loss: 0.4585\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4368 - val_loss: 0.4580\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4355 - val_loss: 0.4562\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4344 - val_loss: 0.4553\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4333 - val_loss: 0.4539\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4323 - val_loss: 0.4527\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4310 - val_loss: 0.4520\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4299 - val_loss: 0.4508\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4290 - val_loss: 0.4501\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4283 - val_loss: 0.4488\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4273 - val_loss: 0.4480\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4264 - val_loss: 0.4470\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4255 - val_loss: 0.4461\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4247 - val_loss: 0.4458\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4239 - val_loss: 0.4448\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4230 - val_loss: 0.4435\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4222 - val_loss: 0.4429\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4215 - val_loss: 0.4424\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4206 - val_loss: 0.4412\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4199 - val_loss: 0.4408\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4191 - val_loss: 0.4398\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4184 - val_loss: 0.4390\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4176 - val_loss: 0.4383\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.4169 - val_loss: 0.4375\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4163 - val_loss: 0.4368\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4156 - val_loss: 0.4365\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4149 - val_loss: 0.4358\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4142 - val_loss: 0.4353\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4136 - val_loss: 0.4341\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4130 - val_loss: 0.4337\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4122 - val_loss: 0.4329\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4116 - val_loss: 0.4323\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4109 - val_loss: 0.4319\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4105 - val_loss: 0.4314\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4096 - val_loss: 0.4307\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4092 - val_loss: 0.4302\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4086 - val_loss: 0.4293\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4079 - val_loss: 0.4288\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4074 - val_loss: 0.4287\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4067 - val_loss: 0.4285\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4059 - val_loss: 0.4274\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4056 - val_loss: 0.4271\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4049 - val_loss: 0.4263\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4044 - val_loss: 0.4255\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4040 - val_loss: 0.4253\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4033 - val_loss: 0.4246\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4027 - val_loss: 0.4239\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4021 - val_loss: 0.4236\n",
      "121/121 [==============================] - 0s 496us/step - loss: 0.3872\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7336 - val_loss: 1.4348\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 1.2363 - val_loss: 0.9997\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.8895 - val_loss: 0.8588\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.7944 - val_loss: 0.7945\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7489 - val_loss: 0.7590\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.7194 - val_loss: 0.7332\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6958 - val_loss: 0.7127\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6758 - val_loss: 0.6942\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6575 - val_loss: 0.6773\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6411 - val_loss: 0.6635\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.6262 - val_loss: 0.6494\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6121 - val_loss: 0.6369\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5990 - val_loss: 0.6261\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5869 - val_loss: 0.6150\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5764 - val_loss: 0.6043\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5662 - val_loss: 0.5949\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5570 - val_loss: 0.5867\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5481 - val_loss: 0.5800\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5405 - val_loss: 0.5717\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5328 - val_loss: 0.5650\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5260 - val_loss: 0.5585\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.5198 - val_loss: 0.5530\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5135 - val_loss: 0.5484\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5086 - val_loss: 0.5429\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5034 - val_loss: 0.5380\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4987 - val_loss: 0.5342\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4944 - val_loss: 0.5303\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4905 - val_loss: 0.5266\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4867 - val_loss: 0.5237\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4831 - val_loss: 0.5206\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4798 - val_loss: 0.5168\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4767 - val_loss: 0.5140\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4737 - val_loss: 0.5111\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4710 - val_loss: 0.5091\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4683 - val_loss: 0.5060\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4660 - val_loss: 0.5040\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4636 - val_loss: 0.5017\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4615 - val_loss: 0.4998\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4592 - val_loss: 0.4975\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4573 - val_loss: 0.4958\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4554 - val_loss: 0.4940\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4535 - val_loss: 0.4920\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4520 - val_loss: 0.4906\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4502 - val_loss: 0.4896\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4487 - val_loss: 0.4878\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4471 - val_loss: 0.4862\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4457 - val_loss: 0.4848\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.447 - 0s 744us/step - loss: 0.4441 - val_loss: 0.4833\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4427 - val_loss: 0.4823\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4414 - val_loss: 0.4812\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4400 - val_loss: 0.4801\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4389 - val_loss: 0.4790\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4376 - val_loss: 0.4781\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4362 - val_loss: 0.4785\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4354 - val_loss: 0.4760\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4341 - val_loss: 0.4747\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4329 - val_loss: 0.4733\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4320 - val_loss: 0.4725\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4308 - val_loss: 0.4714\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4295 - val_loss: 0.4700\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4289 - val_loss: 0.4693\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4278 - val_loss: 0.4686\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4267 - val_loss: 0.4672\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4258 - val_loss: 0.4663\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4250 - val_loss: 0.4657\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4238 - val_loss: 0.4644\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4231 - val_loss: 0.4644\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4222 - val_loss: 0.4634\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4213 - val_loss: 0.4622\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4203 - val_loss: 0.4612\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4194 - val_loss: 0.4602\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4187 - val_loss: 0.4600\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4180 - val_loss: 0.4591\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4170 - val_loss: 0.4581\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4163 - val_loss: 0.4572\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4155 - val_loss: 0.4566\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4148 - val_loss: 0.4566\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4140 - val_loss: 0.4549\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4132 - val_loss: 0.4544\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4124 - val_loss: 0.4534\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4116 - val_loss: 0.4533\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4109 - val_loss: 0.4521\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4102 - val_loss: 0.4518\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4095 - val_loss: 0.4510\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4087 - val_loss: 0.4508\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4080 - val_loss: 0.4502\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4073 - val_loss: 0.4492\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4066 - val_loss: 0.4485\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4058 - val_loss: 0.4488\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4053 - val_loss: 0.4471\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4045 - val_loss: 0.4471\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4039 - val_loss: 0.4458\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4032 - val_loss: 0.4457\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4025 - val_loss: 0.4448\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4019 - val_loss: 0.4438\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4013 - val_loss: 0.4433\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4007 - val_loss: 0.4428\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4001 - val_loss: 0.4422\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3993 - val_loss: 0.4414\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3988 - val_loss: 0.4411\n",
      "121/121 [==============================] - 0s 463us/step - loss: 0.4371\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5097 - val_loss: 1.3290\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 1.1400 - val_loss: 0.8597\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.8763 - val_loss: 0.7960\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7967 - val_loss: 0.7654\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.7584 - val_loss: 0.7515\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7310 - val_loss: 0.7312\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7102 - val_loss: 0.7159\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6929 - val_loss: 0.7036\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6781 - val_loss: 0.6917\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6639 - val_loss: 0.6793\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6513 - val_loss: 0.6689\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6391 - val_loss: 0.6583\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6277 - val_loss: 0.6485\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6165 - val_loss: 0.6382\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6063 - val_loss: 0.6294\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5964 - val_loss: 0.6207\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5869 - val_loss: 0.6135\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5780 - val_loss: 0.6044\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5697 - val_loss: 0.5974\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5618 - val_loss: 0.5907\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5544 - val_loss: 0.5844\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5474 - val_loss: 0.5781\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5410 - val_loss: 0.5725\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5348 - val_loss: 0.5678\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5293 - val_loss: 0.5623\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5241 - val_loss: 0.5580\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5194 - val_loss: 0.5538\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5150 - val_loss: 0.5495\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5107 - val_loss: 0.5465\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5071 - val_loss: 0.5427\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5036 - val_loss: 0.5397\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5003 - val_loss: 0.5368\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4974 - val_loss: 0.5340\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4945 - val_loss: 0.5314\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4920 - val_loss: 0.5292\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4896 - val_loss: 0.5270\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4871 - val_loss: 0.5248\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4851 - val_loss: 0.5228\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4830 - val_loss: 0.5211\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4811 - val_loss: 0.5193\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4794 - val_loss: 0.5175\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4778 - val_loss: 0.5156\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4761 - val_loss: 0.5145\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4748 - val_loss: 0.5131\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4733 - val_loss: 0.5115\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4720 - val_loss: 0.5107\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4708 - val_loss: 0.5088\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4695 - val_loss: 0.5078\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4683 - val_loss: 0.5069\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4671 - val_loss: 0.5055\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.4661 - val_loss: 0.5045\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4650 - val_loss: 0.5038\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4640 - val_loss: 0.5024\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4629 - val_loss: 0.5015\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4620 - val_loss: 0.5002\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4610 - val_loss: 0.4995\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4601 - val_loss: 0.4985\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4591 - val_loss: 0.4977\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4583 - val_loss: 0.4966\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4573 - val_loss: 0.4955\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4562 - val_loss: 0.4950\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4555 - val_loss: 0.4941\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4547 - val_loss: 0.4932\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4538 - val_loss: 0.4921\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4527 - val_loss: 0.4912\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4519 - val_loss: 0.4909\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4513 - val_loss: 0.4897\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4503 - val_loss: 0.4890\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4495 - val_loss: 0.4884\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4488 - val_loss: 0.4874\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4479 - val_loss: 0.4867\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4472 - val_loss: 0.4859\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4465 - val_loss: 0.4851\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4457 - val_loss: 0.4844\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4450 - val_loss: 0.4836\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4442 - val_loss: 0.4829\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4434 - val_loss: 0.4824\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4427 - val_loss: 0.4814\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4422 - val_loss: 0.4806\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4414 - val_loss: 0.4798\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4404 - val_loss: 0.4793\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4402 - val_loss: 0.4784\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4394 - val_loss: 0.4778\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4388 - val_loss: 0.4770\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4381 - val_loss: 0.4765\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4377 - val_loss: 0.4758\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4370 - val_loss: 0.4750\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4364 - val_loss: 0.4745\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4358 - val_loss: 0.4737\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4353 - val_loss: 0.4729\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4346 - val_loss: 0.4726\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4342 - val_loss: 0.4717\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4335 - val_loss: 0.4711\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4330 - val_loss: 0.4704\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4325 - val_loss: 0.4697\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4320 - val_loss: 0.4691\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4314 - val_loss: 0.4687\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4309 - val_loss: 0.4679\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4303 - val_loss: 0.4675\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4298 - val_loss: 0.4665\n",
      "121/121 [==============================] - 0s 430us/step - loss: 0.4510\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7439 - val_loss: 2.4113\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 2.1021 - val_loss: 1.6821\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 1.6148 - val_loss: 1.4057\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 1.3337 - val_loss: 1.1890\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 1.1332 - val_loss: 1.0470\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.9930 - val_loss: 0.9491\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.9009 - val_loss: 0.8825\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.8442 - val_loss: 0.8387\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.8066 - val_loss: 0.8083\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7794 - val_loss: 0.7863\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7586 - val_loss: 0.7668\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.7411 - val_loss: 0.7511\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7259 - val_loss: 0.7381\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.7125 - val_loss: 0.7248\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7006 - val_loss: 0.7144\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6895 - val_loss: 0.7040\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6792 - val_loss: 0.6957\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6697 - val_loss: 0.6859\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6608 - val_loss: 0.6780\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6524 - val_loss: 0.6707\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6444 - val_loss: 0.6624\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6369 - val_loss: 0.6555\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6295 - val_loss: 0.6499\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6229 - val_loss: 0.6434\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6164 - val_loss: 0.6366\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6101 - val_loss: 0.6304\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6041 - val_loss: 0.6238\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5984 - val_loss: 0.6187\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5928 - val_loss: 0.6138\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5875 - val_loss: 0.6080\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5824 - val_loss: 0.6026\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5776 - val_loss: 0.5986\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5729 - val_loss: 0.5940\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5684 - val_loss: 0.5889\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5641 - val_loss: 0.5851\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5599 - val_loss: 0.5805\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5562 - val_loss: 0.5769\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5524 - val_loss: 0.5739\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5487 - val_loss: 0.5695\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5454 - val_loss: 0.5666\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5422 - val_loss: 0.5639\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5388 - val_loss: 0.5596\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5361 - val_loss: 0.5569\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5332 - val_loss: 0.5549\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5303 - val_loss: 0.5517\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5276 - val_loss: 0.5485\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5251 - val_loss: 0.5466\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5225 - val_loss: 0.5436\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5201 - val_loss: 0.5411\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5178 - val_loss: 0.5391\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5155 - val_loss: 0.5370\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5134 - val_loss: 0.5352\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.5113 - val_loss: 0.5327\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5092 - val_loss: 0.5318\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5072 - val_loss: 0.5293\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5054 - val_loss: 0.5278\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5034 - val_loss: 0.5253\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.5018 - val_loss: 0.5238\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5001 - val_loss: 0.5225\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4981 - val_loss: 0.5205\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4967 - val_loss: 0.5190\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4953 - val_loss: 0.5176\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4937 - val_loss: 0.5160\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4922 - val_loss: 0.5148\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4907 - val_loss: 0.5133\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4893 - val_loss: 0.5129\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4879 - val_loss: 0.5117\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4864 - val_loss: 0.5096\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4853 - val_loss: 0.5086\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4840 - val_loss: 0.5077\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4827 - val_loss: 0.5060\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4814 - val_loss: 0.5056\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4803 - val_loss: 0.5040\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4791 - val_loss: 0.5031\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4780 - val_loss: 0.5017\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4767 - val_loss: 0.5005\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4758 - val_loss: 0.4994\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4746 - val_loss: 0.4992\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4736 - val_loss: 0.4977\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4724 - val_loss: 0.4968\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.4715 - val_loss: 0.4956\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4705 - val_loss: 0.4948\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4695 - val_loss: 0.4938\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4683 - val_loss: 0.4929\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4673 - val_loss: 0.4920\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4666 - val_loss: 0.4913\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4654 - val_loss: 0.4903\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.4648 - val_loss: 0.4894\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4638 - val_loss: 0.4886\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4629 - val_loss: 0.4876\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4622 - val_loss: 0.4870\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4613 - val_loss: 0.4866\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4603 - val_loss: 0.4853\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4597 - val_loss: 0.4848\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4588 - val_loss: 0.4841\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4580 - val_loss: 0.4833\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4574 - val_loss: 0.4826\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.4564 - val_loss: 0.4816\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4558 - val_loss: 0.4809\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4549 - val_loss: 0.4802\n",
      "121/121 [==============================] - 0s 496us/step - loss: 0.4426\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8611 - val_loss: 1.6416\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 1.3041 - val_loss: 1.1056\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.9991 - val_loss: 0.9512\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.8772 - val_loss: 0.8624\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.8067 - val_loss: 0.8069\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7615 - val_loss: 0.7691\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.7297 - val_loss: 0.7421\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.7051 - val_loss: 0.7205\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6852 - val_loss: 0.7028\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6683 - val_loss: 0.6888\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6535 - val_loss: 0.6756\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6401 - val_loss: 0.6640\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6277 - val_loss: 0.6543\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6166 - val_loss: 0.6444\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6066 - val_loss: 0.6350\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5972 - val_loss: 0.6267\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5885 - val_loss: 0.6195\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5805 - val_loss: 0.6133\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5733 - val_loss: 0.6061\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5664 - val_loss: 0.6002\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5600 - val_loss: 0.5944\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5542 - val_loss: 0.5894\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5483 - val_loss: 0.5853\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5433 - val_loss: 0.5801\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5383 - val_loss: 0.5754\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5337 - val_loss: 0.5718\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5293 - val_loss: 0.5679\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5253 - val_loss: 0.5642\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5215 - val_loss: 0.5612\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5179 - val_loss: 0.5580\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5145 - val_loss: 0.5545\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5113 - val_loss: 0.5514\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5082 - val_loss: 0.5484\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5053 - val_loss: 0.5462\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5025 - val_loss: 0.5430\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5000 - val_loss: 0.5407\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4975 - val_loss: 0.5382\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4951 - val_loss: 0.5358\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4926 - val_loss: 0.5333\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4906 - val_loss: 0.5312\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4885 - val_loss: 0.5289\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4863 - val_loss: 0.5268\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4845 - val_loss: 0.5248\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4825 - val_loss: 0.5234\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4807 - val_loss: 0.5212\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4788 - val_loss: 0.5194\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4772 - val_loss: 0.5176\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4754 - val_loss: 0.5158\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4738 - val_loss: 0.5147\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4723 - val_loss: 0.5131\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4708 - val_loss: 0.5114\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4693 - val_loss: 0.5100\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4678 - val_loss: 0.5090\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4663 - val_loss: 0.5088\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4652 - val_loss: 0.5062\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4639 - val_loss: 0.5048\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4625 - val_loss: 0.5028\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4614 - val_loss: 0.5018\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4600 - val_loss: 0.5005\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4587 - val_loss: 0.4988\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4577 - val_loss: 0.4979\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4565 - val_loss: 0.4969\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4553 - val_loss: 0.4954\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4542 - val_loss: 0.4943\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4532 - val_loss: 0.4934\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4520 - val_loss: 0.4919\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4511 - val_loss: 0.4916\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4500 - val_loss: 0.4903\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4491 - val_loss: 0.4892\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4481 - val_loss: 0.4880\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4470 - val_loss: 0.4870\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4463 - val_loss: 0.4866\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4454 - val_loss: 0.4858\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4444 - val_loss: 0.4845\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4436 - val_loss: 0.4835\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4428 - val_loss: 0.4828\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4419 - val_loss: 0.4830\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4411 - val_loss: 0.4810\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4403 - val_loss: 0.4807\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4394 - val_loss: 0.4793\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4387 - val_loss: 0.4789\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4379 - val_loss: 0.4778\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4371 - val_loss: 0.4773\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4363 - val_loss: 0.4764\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4355 - val_loss: 0.4761\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4348 - val_loss: 0.4752\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4341 - val_loss: 0.4740\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4334 - val_loss: 0.4732\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4325 - val_loss: 0.4740\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4321 - val_loss: 0.4721\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4313 - val_loss: 0.4719\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4307 - val_loss: 0.4703\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4301 - val_loss: 0.4705\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4293 - val_loss: 0.4693\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4288 - val_loss: 0.4684\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4282 - val_loss: 0.4683\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4276 - val_loss: 0.4677\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4270 - val_loss: 0.4667\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4261 - val_loss: 0.4660\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4258 - val_loss: 0.4657\n",
      "121/121 [==============================] - 0s 463us/step - loss: 0.4543\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 2.0823 - val_loss: 0.7722\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.7115 - val_loss: 0.6637\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6119 - val_loss: 0.6334\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5805 - val_loss: 0.5996\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5678 - val_loss: 0.5923\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5541 - val_loss: 0.5708\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5464 - val_loss: 0.5766\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5381 - val_loss: 0.5619\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5463 - val_loss: 0.6765\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5462 - val_loss: 0.6313\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5403 - val_loss: 0.5560\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5364 - val_loss: 0.5553\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5414 - val_loss: 0.5555\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5269 - val_loss: 0.6987\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5400 - val_loss: 0.6172\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5405 - val_loss: 0.6473\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5397 - val_loss: 0.5546\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5388 - val_loss: 0.5820\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5302 - val_loss: 0.6868\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5446 - val_loss: 0.5653\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5310 - val_loss: 0.6579\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5238 - val_loss: 0.5484\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5410 - val_loss: 0.5539\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.5354 - val_loss: 0.6196\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5199 - val_loss: 0.5464\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5375 - val_loss: 0.5487\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5323 - val_loss: 0.5463\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.5466 - val_loss: 0.5570\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5268 - val_loss: 0.5447\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5454 - val_loss: 0.5474\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5369 - val_loss: 0.5544\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5362 - val_loss: 0.5503\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5299 - val_loss: 0.7844\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5447 - val_loss: 0.5511\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5381 - val_loss: 0.5554\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5334 - val_loss: 0.5461\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5293 - val_loss: 0.6265\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5413 - val_loss: 0.5498\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5316 - val_loss: 0.5870\n",
      "121/121 [==============================] - 0s 462us/step - loss: 0.5398\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 1.4969 - val_loss: 0.7154\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6206 - val_loss: 0.6420\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5978 - val_loss: 0.6143\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5794 - val_loss: 0.5837\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5877 - val_loss: 0.5832\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5587 - val_loss: 0.5665\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5607 - val_loss: 0.5871\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5490 - val_loss: 0.5569\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5571 - val_loss: 0.6732\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5573 - val_loss: 0.6746\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5528 - val_loss: 0.5552\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5595 - val_loss: 0.5551\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5541 - val_loss: 0.5564\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5474 - val_loss: 0.6387\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5481 - val_loss: 0.6006\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5506 - val_loss: 0.6535\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5476 - val_loss: 0.5514\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5517 - val_loss: 0.6125\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5434 - val_loss: 0.6729\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5543 - val_loss: 0.5662\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5391 - val_loss: 0.6800\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5382 - val_loss: 0.5465\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5537 - val_loss: 0.5498\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5526 - val_loss: 0.5649\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5321 - val_loss: 0.5463\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5448 - val_loss: 0.5475\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5444 - val_loss: 0.5461\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5522 - val_loss: 0.5524\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5381 - val_loss: 0.5449\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5546 - val_loss: 0.5467\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5403 - val_loss: 0.5449\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5549 - val_loss: 0.5518\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5373 - val_loss: 0.7049\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5534 - val_loss: 0.5515\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5486 - val_loss: 0.5526\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5429 - val_loss: 0.5448\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5464 - val_loss: 0.6197\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5455 - val_loss: 0.5491\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5429 - val_loss: 0.5990\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5424 - val_loss: 0.5987\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5477 - val_loss: 0.5753\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5440 - val_loss: 0.5464\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5520 - val_loss: 0.5531\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5466 - val_loss: 0.5465\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5533 - val_loss: 0.5499\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5406 - val_loss: 0.5440\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5474 - val_loss: 0.6113\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5430 - val_loss: 0.5457\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5521 - val_loss: 0.5481\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5417 - val_loss: 0.6301\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5453 - val_loss: 0.6598\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5413 - val_loss: 0.5449\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5525 - val_loss: 0.6078\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5520 - val_loss: 0.5651\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5356 - val_loss: 0.6723\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5430 - val_loss: 0.5440\n",
      "121/121 [==============================] - 0s 463us/step - loss: 0.5036\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 1.7843 - val_loss: 0.6398\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5674 - val_loss: 0.5901\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5700 - val_loss: 0.5895\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6455 - val_loss: 0.5511\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 1.2663 - val_loss: 0.6217\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 4.1214 - val_loss: 0.5152\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 21.6564 - val_loss: 1.2179\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 113.5895 - val_loss: 2.3352\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 611.6547 - val_loss: 28.1923\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 3578.9324 - val_loss: 67.6956\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 19280.9531 - val_loss: 675.9553\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 110403.1250 - val_loss: 2587.2581\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 624454.2500 - val_loss: 12084.6963\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 3486006.7500 - val_loss: 65304.5352\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 19023756.0000 - val_loss: 377285.0312\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 108884984.0000 - val_loss: 1970060.6250\n",
      "121/121 [==============================] - 0s 429us/step - loss: 321827392.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 1.6281 - val_loss: 0.7174\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.8838 - val_loss: 0.5407\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4819 - val_loss: 0.4977\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4508 - val_loss: 0.4725\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4350 - val_loss: 0.4640\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4277 - val_loss: 0.4523\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4186 - val_loss: 0.4426\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4116 - val_loss: 0.4388\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4077 - val_loss: 0.4336\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4039 - val_loss: 0.4315\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4058 - val_loss: 0.4234\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3935 - val_loss: 0.4162\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3915 - val_loss: 0.4106\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3882 - val_loss: 0.4530\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3828 - val_loss: 0.4019\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3879 - val_loss: 0.4474\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3795 - val_loss: 0.3999\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3759 - val_loss: 0.4030\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3720 - val_loss: 0.4630\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3705 - val_loss: 0.3915\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3622 - val_loss: 0.3919\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3593 - val_loss: 0.3936\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3566 - val_loss: 0.3813\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3542 - val_loss: 0.3875\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3644 - val_loss: 0.3764\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3579 - val_loss: 0.3775\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3503 - val_loss: 0.3753\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3540 - val_loss: 0.3751\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3474 - val_loss: 0.3735\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3523 - val_loss: 0.3728\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3437 - val_loss: 0.3724\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3393 - val_loss: 0.3673\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3376 - val_loss: 0.3844\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3360 - val_loss: 0.3660\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3335 - val_loss: 0.3687\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3336 - val_loss: 0.3631\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3330 - val_loss: 0.3671\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3349 - val_loss: 0.3623\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3325 - val_loss: 0.3679\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3288 - val_loss: 0.3590\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3266 - val_loss: 0.3571\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3348 - val_loss: 0.3643\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3294 - val_loss: 0.3605\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3247 - val_loss: 0.3612\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3226 - val_loss: 0.3583\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3306 - val_loss: 0.3563\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3228 - val_loss: 0.3572\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3216 - val_loss: 0.3532\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3220 - val_loss: 0.3548\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3198 - val_loss: 0.3542\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3205 - val_loss: 0.3617\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3190 - val_loss: 0.3512\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3165 - val_loss: 0.3559\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3284 - val_loss: 0.3485\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3188 - val_loss: 0.3564\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3403 - val_loss: 0.3527\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3167 - val_loss: 0.3489\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3174 - val_loss: 0.3511\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3151 - val_loss: 0.3455\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3151 - val_loss: 0.3494\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3155 - val_loss: 0.3467\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3130 - val_loss: 0.3457\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3126 - val_loss: 0.3446\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3147 - val_loss: 0.3469\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3145 - val_loss: 0.3511\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3127 - val_loss: 0.3490\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3100 - val_loss: 0.3791\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3205 - val_loss: 0.3461\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3174 - val_loss: 0.3487\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3118 - val_loss: 0.3574\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3086 - val_loss: 0.3430\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3141 - val_loss: 0.3437\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3074 - val_loss: 0.3493\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3076 - val_loss: 0.3456\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3086 - val_loss: 0.3413\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3066 - val_loss: 0.3475\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3062 - val_loss: 0.3420\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3068 - val_loss: 0.3461\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3056 - val_loss: 0.3464\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3037 - val_loss: 0.3481\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3053 - val_loss: 0.3546\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3347 - val_loss: 0.3442\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3055 - val_loss: 0.3402\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3048 - val_loss: 0.3405\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3053 - val_loss: 0.3430\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3033 - val_loss: 0.3380\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3045 - val_loss: 0.3395\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3035 - val_loss: 0.3469\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3036 - val_loss: 0.3404\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3030 - val_loss: 0.3394\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3030 - val_loss: 0.3386\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3023 - val_loss: 0.3378\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3013 - val_loss: 0.3386\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.3011 - val_loss: 0.3358\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3026 - val_loss: 0.3344\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3021 - val_loss: 0.3397\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3018 - val_loss: 0.3381\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3005 - val_loss: 0.3381\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.2991 - val_loss: 0.3347\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3050 - val_loss: 0.3450\n",
      "121/121 [==============================] - 0s 463us/step - loss: 0.3222\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 1.0208 - val_loss: 0.6385\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6084 - val_loss: 0.5544\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5231 - val_loss: 0.5209\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4924 - val_loss: 0.4992\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4850 - val_loss: 0.4907\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4659 - val_loss: 0.4744\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4578 - val_loss: 0.4694\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4525 - val_loss: 0.4613\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4461 - val_loss: 0.4594\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4422 - val_loss: 0.4654\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4448 - val_loss: 0.4578\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4386 - val_loss: 0.4477\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4312 - val_loss: 0.4467\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4307 - val_loss: 0.4413\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4286 - val_loss: 0.4551\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4228 - val_loss: 0.4357\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4231 - val_loss: 0.4359\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4163 - val_loss: 0.4301\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4143 - val_loss: 0.4385\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4126 - val_loss: 0.4270\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4088 - val_loss: 0.4330\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4062 - val_loss: 0.4299\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4040 - val_loss: 0.4232\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4022 - val_loss: 0.4169\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4013 - val_loss: 0.4133\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4022 - val_loss: 0.4190\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3972 - val_loss: 0.4148\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3955 - val_loss: 0.4113\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3930 - val_loss: 0.4095\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3997 - val_loss: 0.4192\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4226 - val_loss: 0.4059\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3888 - val_loss: 0.4058\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3878 - val_loss: 0.4832\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3952 - val_loss: 0.4066\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3976 - val_loss: 0.3988\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3877 - val_loss: 0.3979\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3822 - val_loss: 0.3980\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3818 - val_loss: 0.3985\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3930 - val_loss: 0.4260\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3854 - val_loss: 0.4002\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3968 - val_loss: 0.3963\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3774 - val_loss: 0.3971\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3762 - val_loss: 0.3920\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3855 - val_loss: 0.3932\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3762 - val_loss: 0.3912\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3834 - val_loss: 0.3889\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3753 - val_loss: 0.3866\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3720 - val_loss: 0.3870\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3752 - val_loss: 0.3889\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3691 - val_loss: 0.3948\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3705 - val_loss: 0.4072\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3685 - val_loss: 0.3921\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3749 - val_loss: 0.3878\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3787 - val_loss: 0.3909\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3665 - val_loss: 0.4873\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5335 - val_loss: 0.3891\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3754 - val_loss: 0.3900\n",
      "121/121 [==============================] - 0s 430us/step - loss: 0.3565\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 1.1343 - val_loss: 0.5684\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 6.9567 - val_loss: 0.4998\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4218 - val_loss: 0.4487\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3913 - val_loss: 0.4224\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3774 - val_loss: 0.4107\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3712 - val_loss: 0.4077\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3662 - val_loss: 0.4003\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3623 - val_loss: 0.3992\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3596 - val_loss: 0.3942\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3575 - val_loss: 0.3930\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3557 - val_loss: 0.3956\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3554 - val_loss: 0.3905\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3517 - val_loss: 0.3964\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3493 - val_loss: 0.3898\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3492 - val_loss: 0.3974\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3493 - val_loss: 0.3851\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3458 - val_loss: 0.3931\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3444 - val_loss: 0.3841\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3433 - val_loss: 0.3849\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3430 - val_loss: 0.3784\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3418 - val_loss: 0.3780\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3402 - val_loss: 0.3755\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3374 - val_loss: 0.3732\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3390 - val_loss: 0.3740\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3358 - val_loss: 0.3774\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3351 - val_loss: 0.3736\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3344 - val_loss: 0.3799\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3343 - val_loss: 0.3726\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3332 - val_loss: 0.3729\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3334 - val_loss: 0.3867\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3310 - val_loss: 0.3691\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3316 - val_loss: 0.3750\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3309 - val_loss: 0.3719\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3295 - val_loss: 0.3725\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3284 - val_loss: 0.3773\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3277 - val_loss: 0.3675\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3282 - val_loss: 0.3633\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3276 - val_loss: 0.3630\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3268 - val_loss: 0.3654\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3253 - val_loss: 0.3657\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3251 - val_loss: 0.3604\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3245 - val_loss: 0.3655\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3250 - val_loss: 0.3637\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3229 - val_loss: 0.3665\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3242 - val_loss: 0.3677\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3226 - val_loss: 0.3587\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3227 - val_loss: 0.3624\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3216 - val_loss: 0.3591\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3218 - val_loss: 0.3580\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3204 - val_loss: 0.3613\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3193 - val_loss: 0.3693\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3196 - val_loss: 0.3655\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3189 - val_loss: 0.3623\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3179 - val_loss: 0.3775\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3186 - val_loss: 0.3583\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3182 - val_loss: 0.3616\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3176 - val_loss: 0.3632\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3177 - val_loss: 0.3580\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3169 - val_loss: 0.3591\n",
      "121/121 [==============================] - 0s 496us/step - loss: 0.3632\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0271 - val_loss: 2.8714\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 2.3241 - val_loss: 1.7888\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 1.6467 - val_loss: 1.4085\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 1.3353 - val_loss: 1.2308\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 1.1749 - val_loss: 1.1298\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 1.0679 - val_loss: 1.0457\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.9888 - val_loss: 0.9800\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.9249 - val_loss: 0.9255\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.8727 - val_loss: 0.8795\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.8291 - val_loss: 0.8403\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7928 - val_loss: 0.8075\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7629 - val_loss: 0.7807\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7388 - val_loss: 0.7576\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7186 - val_loss: 0.7381\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7022 - val_loss: 0.7224\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6886 - val_loss: 0.7090\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6770 - val_loss: 0.6980\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6672 - val_loss: 0.6876\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6586 - val_loss: 0.6790\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6510 - val_loss: 0.6714\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6440 - val_loss: 0.6644\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6376 - val_loss: 0.6579\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6317 - val_loss: 0.6518\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6260 - val_loss: 0.6467\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6208 - val_loss: 0.6413\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6158 - val_loss: 0.6366\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6110 - val_loss: 0.6319\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6065 - val_loss: 0.6273\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6020 - val_loss: 0.6234\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5978 - val_loss: 0.6191\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5937 - val_loss: 0.6154\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5899 - val_loss: 0.6116\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5862 - val_loss: 0.6081\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5825 - val_loss: 0.6045\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5789 - val_loss: 0.6013\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5754 - val_loss: 0.5978\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5720 - val_loss: 0.5948\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5687 - val_loss: 0.5915\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5655 - val_loss: 0.5889\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5624 - val_loss: 0.5858\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5592 - val_loss: 0.5832\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5563 - val_loss: 0.5801\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5533 - val_loss: 0.5776\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5506 - val_loss: 0.5748\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5476 - val_loss: 0.5724\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5450 - val_loss: 0.5697\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5423 - val_loss: 0.5670\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5395 - val_loss: 0.5644\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5369 - val_loss: 0.5620\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5344 - val_loss: 0.5596\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5318 - val_loss: 0.5572\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5294 - val_loss: 0.5550\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5270 - val_loss: 0.5528\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5246 - val_loss: 0.5504\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.5223 - val_loss: 0.5483\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5200 - val_loss: 0.5460\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5178 - val_loss: 0.5439\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5156 - val_loss: 0.5419\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5134 - val_loss: 0.5399\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5114 - val_loss: 0.5379\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5092 - val_loss: 0.5358\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5072 - val_loss: 0.5340\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5053 - val_loss: 0.5321\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5032 - val_loss: 0.5305\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5013 - val_loss: 0.5285\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4994 - val_loss: 0.5271\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4977 - val_loss: 0.5252\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4957 - val_loss: 0.5234\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4940 - val_loss: 0.5218\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4924 - val_loss: 0.5203\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4906 - val_loss: 0.5187\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4889 - val_loss: 0.5173\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4874 - val_loss: 0.5157\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4857 - val_loss: 0.5143\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4842 - val_loss: 0.5129\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4827 - val_loss: 0.5114\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4811 - val_loss: 0.5102\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4797 - val_loss: 0.5088\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4784 - val_loss: 0.5074\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4769 - val_loss: 0.5061\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4754 - val_loss: 0.5051\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4742 - val_loss: 0.5038\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4729 - val_loss: 0.5025\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4716 - val_loss: 0.5014\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4703 - val_loss: 0.5003\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4692 - val_loss: 0.4992\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4681 - val_loss: 0.4981\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4670 - val_loss: 0.4971\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4658 - val_loss: 0.4961\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4648 - val_loss: 0.4952\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4636 - val_loss: 0.4942\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4627 - val_loss: 0.4932\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4616 - val_loss: 0.4924\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4607 - val_loss: 0.4915\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4597 - val_loss: 0.4906\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4588 - val_loss: 0.4898\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4579 - val_loss: 0.4890\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4570 - val_loss: 0.4882\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4560 - val_loss: 0.4875\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4553 - val_loss: 0.4866\n",
      "121/121 [==============================] - 0s 463us/step - loss: 0.4713\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0199 - val_loss: 2.9999\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 2.1039 - val_loss: 1.4515\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 1.2682 - val_loss: 1.1152\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 1.0662 - val_loss: 0.9975\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.9668 - val_loss: 0.9341\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.9002 - val_loss: 0.8914\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.8532 - val_loss: 0.8591\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.8190 - val_loss: 0.8343\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7942 - val_loss: 0.8150\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.7740 - val_loss: 0.7985\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7574 - val_loss: 0.7828\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.7430 - val_loss: 0.7695\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7304 - val_loss: 0.7577\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7193 - val_loss: 0.7458\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.7093 - val_loss: 0.7358\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.7001 - val_loss: 0.7263\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6917 - val_loss: 0.7179\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6840 - val_loss: 0.7093\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6769 - val_loss: 0.7019\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6702 - val_loss: 0.6949\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6640 - val_loss: 0.6881\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6582 - val_loss: 0.6820\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6526 - val_loss: 0.6765\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6475 - val_loss: 0.6714\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6426 - val_loss: 0.6659\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6379 - val_loss: 0.6610\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6334 - val_loss: 0.6558\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6292 - val_loss: 0.6514\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6251 - val_loss: 0.6476\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6213 - val_loss: 0.6430\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6176 - val_loss: 0.6390\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6139 - val_loss: 0.6354\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6105 - val_loss: 0.6317\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.6070 - val_loss: 0.6278\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6037 - val_loss: 0.6248\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6005 - val_loss: 0.6209\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5975 - val_loss: 0.6176\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5944 - val_loss: 0.6148\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5913 - val_loss: 0.6112\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5883 - val_loss: 0.6081\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5853 - val_loss: 0.6052\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5819 - val_loss: 0.6013\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5793 - val_loss: 0.5985\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5762 - val_loss: 0.5962\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5733 - val_loss: 0.5928\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5705 - val_loss: 0.5894\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5677 - val_loss: 0.5867\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5648 - val_loss: 0.5835\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5621 - val_loss: 0.5806\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5592 - val_loss: 0.5779\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5565 - val_loss: 0.5752\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5538 - val_loss: 0.5727\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5512 - val_loss: 0.5696\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5484 - val_loss: 0.5675\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5458 - val_loss: 0.5643\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5433 - val_loss: 0.5621\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5407 - val_loss: 0.5588\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5383 - val_loss: 0.5563\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5358 - val_loss: 0.5541\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5334 - val_loss: 0.5511\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5311 - val_loss: 0.5487\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5288 - val_loss: 0.5463\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5266 - val_loss: 0.5440\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5243 - val_loss: 0.5419\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5221 - val_loss: 0.5395\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5200 - val_loss: 0.5376\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5178 - val_loss: 0.5357\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5156 - val_loss: 0.5330\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5137 - val_loss: 0.5314\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5116 - val_loss: 0.5294\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5095 - val_loss: 0.5267\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5075 - val_loss: 0.5254\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5055 - val_loss: 0.5229\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5037 - val_loss: 0.5211\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5018 - val_loss: 0.5190\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4999 - val_loss: 0.5172\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4983 - val_loss: 0.5154\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4965 - val_loss: 0.5141\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4948 - val_loss: 0.5121\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4931 - val_loss: 0.5107\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4915 - val_loss: 0.5090\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4899 - val_loss: 0.5074\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4883 - val_loss: 0.5056\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4866 - val_loss: 0.5043\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4851 - val_loss: 0.5025\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4838 - val_loss: 0.5015\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4822 - val_loss: 0.4999\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4810 - val_loss: 0.4987\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4796 - val_loss: 0.4972\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4782 - val_loss: 0.4957\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4770 - val_loss: 0.4946\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4758 - val_loss: 0.4938\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4743 - val_loss: 0.4921\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4734 - val_loss: 0.4913\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4722 - val_loss: 0.4898\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4711 - val_loss: 0.4888\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4702 - val_loss: 0.4876\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4690 - val_loss: 0.4862\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4681 - val_loss: 0.4856\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4671 - val_loss: 0.4843\n",
      "121/121 [==============================] - 0s 462us/step - loss: 0.4429\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3177 - val_loss: 3.3653\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 2.5509 - val_loss: 1.8662\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 1.4147 - val_loss: 1.1733\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.9961 - val_loss: 0.9637\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.8593 - val_loss: 0.8748\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7967 - val_loss: 0.8216\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7585 - val_loss: 0.7848\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.7319 - val_loss: 0.7577\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7127 - val_loss: 0.7373\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6980 - val_loss: 0.7213\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6858 - val_loss: 0.7081\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6753 - val_loss: 0.6971\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6660 - val_loss: 0.6878\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6577 - val_loss: 0.6796\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6503 - val_loss: 0.6716\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6433 - val_loss: 0.6645\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6367 - val_loss: 0.6582\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.6304 - val_loss: 0.6527\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6244 - val_loss: 0.6468\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6186 - val_loss: 0.6414\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6131 - val_loss: 0.6362\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.6078 - val_loss: 0.6316\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6025 - val_loss: 0.6276\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5976 - val_loss: 0.6228\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5927 - val_loss: 0.6182\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5880 - val_loss: 0.6141\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5833 - val_loss: 0.6099\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5787 - val_loss: 0.6059\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5741 - val_loss: 0.6021\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5695 - val_loss: 0.5980\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5650 - val_loss: 0.5936\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5604 - val_loss: 0.5894\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5560 - val_loss: 0.5853\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5514 - val_loss: 0.5817\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5470 - val_loss: 0.5772\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5425 - val_loss: 0.5731\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5380 - val_loss: 0.5690\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5333 - val_loss: 0.5651\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5286 - val_loss: 0.5605\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5243 - val_loss: 0.5565\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5200 - val_loss: 0.5529\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5157 - val_loss: 0.5488\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5119 - val_loss: 0.5455\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5080 - val_loss: 0.5424\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5043 - val_loss: 0.5386\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5007 - val_loss: 0.5352\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4973 - val_loss: 0.5322\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4940 - val_loss: 0.5291\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4908 - val_loss: 0.5262\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4877 - val_loss: 0.5234\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4847 - val_loss: 0.5209\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4818 - val_loss: 0.5184\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4791 - val_loss: 0.5158\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4762 - val_loss: 0.5141\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4739 - val_loss: 0.5112\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4713 - val_loss: 0.5089\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4689 - val_loss: 0.5063\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4666 - val_loss: 0.5044\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4643 - val_loss: 0.5024\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4620 - val_loss: 0.5000\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4601 - val_loss: 0.4982\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4580 - val_loss: 0.4965\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4560 - val_loss: 0.4945\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4540 - val_loss: 0.4927\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4521 - val_loss: 0.4911\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4502 - val_loss: 0.4893\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4486 - val_loss: 0.4881\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4469 - val_loss: 0.4868\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4452 - val_loss: 0.4850\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4437 - val_loss: 0.4835\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4420 - val_loss: 0.4820\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4408 - val_loss: 0.4812\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4394 - val_loss: 0.4799\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4380 - val_loss: 0.4786\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4367 - val_loss: 0.4772\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4355 - val_loss: 0.4764\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4343 - val_loss: 0.4755\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.4331 - val_loss: 0.4740\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4320 - val_loss: 0.4731\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4308 - val_loss: 0.4718\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4297 - val_loss: 0.4710\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.4287 - val_loss: 0.4701\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4277 - val_loss: 0.4691\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4266 - val_loss: 0.4680\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.4256 - val_loss: 0.4674\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4247 - val_loss: 0.4666\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4238 - val_loss: 0.4653\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4228 - val_loss: 0.4645\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4218 - val_loss: 0.4641\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4211 - val_loss: 0.4630\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4202 - val_loss: 0.4623\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4194 - val_loss: 0.4611\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4185 - val_loss: 0.4607\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4176 - val_loss: 0.4597\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4169 - val_loss: 0.4588\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4161 - val_loss: 0.4581\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4153 - val_loss: 0.4575\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4146 - val_loss: 0.4568\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4137 - val_loss: 0.4559\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4131 - val_loss: 0.4555\n",
      "121/121 [==============================] - 0s 463us/step - loss: 0.4568\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 1.2859 - val_loss: 0.7048\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.7302 - val_loss: 0.6021\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6199 - val_loss: 0.5794\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5359 - val_loss: 0.5408\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5114 - val_loss: 0.5247\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4928 - val_loss: 0.5084\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4844 - val_loss: 0.5053\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4737 - val_loss: 0.4951\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4687 - val_loss: 0.4990\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4655 - val_loss: 0.4964\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4608 - val_loss: 0.4843\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4556 - val_loss: 0.4794\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4548 - val_loss: 0.4765\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4498 - val_loss: 0.4757\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4465 - val_loss: 0.4709\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4467 - val_loss: 0.4895\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4435 - val_loss: 0.4685\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4396 - val_loss: 0.4632\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4373 - val_loss: 0.4611\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4351 - val_loss: 0.4615\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4329 - val_loss: 0.4595\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4303 - val_loss: 0.4566\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4314 - val_loss: 0.4559\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4270 - val_loss: 0.4550\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4253 - val_loss: 0.4485\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4228 - val_loss: 0.4484\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4209 - val_loss: 0.4470\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4225 - val_loss: 0.4471\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4208 - val_loss: 0.4442\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4164 - val_loss: 0.4425\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4145 - val_loss: 0.4394\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4144 - val_loss: 0.4387\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4119 - val_loss: 0.4377\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4096 - val_loss: 0.4359\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4092 - val_loss: 0.4362\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4077 - val_loss: 0.4310\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4071 - val_loss: 0.4459\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4076 - val_loss: 0.4326\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4062 - val_loss: 0.4404\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4038 - val_loss: 0.4388\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4033 - val_loss: 0.4286\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4155 - val_loss: 0.4346\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4060 - val_loss: 0.4309\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3998 - val_loss: 0.4262\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3994 - val_loss: 0.4233\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4002 - val_loss: 0.4231\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3955 - val_loss: 0.4209\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3953 - val_loss: 0.4215\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3952 - val_loss: 0.4229\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3934 - val_loss: 0.4221\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3949 - val_loss: 0.4421\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3933 - val_loss: 0.4176\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3905 - val_loss: 0.4195\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3922 - val_loss: 0.4163\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3879 - val_loss: 0.4327\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4005 - val_loss: 0.4130\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3872 - val_loss: 0.4145\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3893 - val_loss: 0.4136\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3870 - val_loss: 0.4100\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3939 - val_loss: 0.4164\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3893 - val_loss: 0.4135\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3840 - val_loss: 0.4111\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3846 - val_loss: 0.4078\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3815 - val_loss: 0.4056\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3810 - val_loss: 0.4134\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3786 - val_loss: 0.4104\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3780 - val_loss: 0.4294\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.4184 - val_loss: 0.4168\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3895 - val_loss: 0.4141\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3860 - val_loss: 0.4125\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3783 - val_loss: 0.4061\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3902 - val_loss: 0.4036\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3754 - val_loss: 0.4099\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3743 - val_loss: 0.4019\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3774 - val_loss: 0.3999\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3727 - val_loss: 0.3976\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3720 - val_loss: 0.4012\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3706 - val_loss: 0.3981\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3709 - val_loss: 0.3983\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3686 - val_loss: 0.4017\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3703 - val_loss: 0.4081\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4758 - val_loss: 0.4171\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3791 - val_loss: 0.4129\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3766 - val_loss: 0.4046\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3712 - val_loss: 0.4011\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3719 - val_loss: 0.3979\n",
      "  1/121 [..............................] - ETA: 0s - loss: 0.1852WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0040s). Check your callbacks.\n",
      "121/121 [==============================] - 0s 463us/step - loss: 0.3765\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 1.2558 - val_loss: 0.7089\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.6225 - val_loss: 0.5995\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5712 - val_loss: 0.5617\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5373 - val_loss: 0.5426\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5354 - val_loss: 0.5274\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4972 - val_loss: 0.5113\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4875 - val_loss: 0.5029\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4857 - val_loss: 0.4940\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4751 - val_loss: 0.5029\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4686 - val_loss: 0.4827\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4676 - val_loss: 0.4825\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4613 - val_loss: 0.4747\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4565 - val_loss: 0.4744\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4545 - val_loss: 0.4671\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4500 - val_loss: 0.4680\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4542 - val_loss: 0.5078\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4530 - val_loss: 0.4648\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4431 - val_loss: 0.4578\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4405 - val_loss: 0.4539\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4392 - val_loss: 0.4532\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4355 - val_loss: 0.4506\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4336 - val_loss: 0.4509\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4316 - val_loss: 0.4505\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4318 - val_loss: 0.4467\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4337 - val_loss: 0.4415\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4345 - val_loss: 0.4431\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4244 - val_loss: 0.4409\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4229 - val_loss: 0.4392\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4221 - val_loss: 0.4368\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4287 - val_loss: 0.4425\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4315 - val_loss: 0.4346\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4206 - val_loss: 0.4334\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4163 - val_loss: 0.5216\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4253 - val_loss: 0.4325\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4243 - val_loss: 0.4294\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4106 - val_loss: 0.4261\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4106 - val_loss: 0.4255\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4092 - val_loss: 0.4255\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4168 - val_loss: 0.4509\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4069 - val_loss: 0.4226\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4062 - val_loss: 0.4201\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4089 - val_loss: 0.4204\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4062 - val_loss: 0.4207\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4827 - val_loss: 0.4370\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4154 - val_loss: 0.4273\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4202 - val_loss: 0.4219\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4202 - val_loss: 0.4300\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4074 - val_loss: 0.4167\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4003 - val_loss: 0.4156\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3973 - val_loss: 0.4164\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3980 - val_loss: 0.4588\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3967 - val_loss: 0.4153\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3963 - val_loss: 0.4111\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3971 - val_loss: 0.4113\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3908 - val_loss: 0.4935\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.7236 - val_loss: 0.4591\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4207 - val_loss: 0.4427\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4169 - val_loss: 0.4330\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4071 - val_loss: 0.4255\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4044 - val_loss: 0.4241\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4045 - val_loss: 0.4176\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3977 - val_loss: 0.4128\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3898 - val_loss: 0.4152\n",
      "121/121 [==============================] - 0s 429us/step - loss: 0.3766\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 1.4630 - val_loss: 0.6442\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 2.9174 - val_loss: 0.6516\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6316 - val_loss: 0.5880\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5272 - val_loss: 0.5252\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4687 - val_loss: 0.4835\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4339 - val_loss: 0.4583\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4163 - val_loss: 0.4470\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4059 - val_loss: 0.4381\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4003 - val_loss: 0.4296\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3955 - val_loss: 0.4274\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3923 - val_loss: 0.4261\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3898 - val_loss: 0.4278\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3869 - val_loss: 0.4273\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3839 - val_loss: 0.4205\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3834 - val_loss: 0.4252\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3822 - val_loss: 0.4175\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3799 - val_loss: 0.4206\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3787 - val_loss: 0.4162\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3775 - val_loss: 0.4113\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3768 - val_loss: 0.4112\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3754 - val_loss: 0.4107\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3750 - val_loss: 0.4097\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3723 - val_loss: 0.4073\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3729 - val_loss: 0.4074\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3714 - val_loss: 0.4067\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3699 - val_loss: 0.4080\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3690 - val_loss: 0.4130\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3690 - val_loss: 0.4047\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.3682 - val_loss: 0.4055\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3677 - val_loss: 0.4110\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3663 - val_loss: 0.4051\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3662 - val_loss: 0.4065\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3655 - val_loss: 0.4053\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3641 - val_loss: 0.4053\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3632 - val_loss: 0.4056\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3629 - val_loss: 0.4007\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3625 - val_loss: 0.3984\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3627 - val_loss: 0.4002\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3616 - val_loss: 0.3992\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3611 - val_loss: 0.4025\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3597 - val_loss: 0.3974\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3593 - val_loss: 0.3987\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3584 - val_loss: 0.3964\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3576 - val_loss: 0.4009\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3587 - val_loss: 0.3999\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3570 - val_loss: 0.3959\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3572 - val_loss: 0.3970\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3568 - val_loss: 0.3957\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3560 - val_loss: 0.3941\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3555 - val_loss: 0.3947\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3548 - val_loss: 0.3993\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3544 - val_loss: 0.3978\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3543 - val_loss: 0.3957\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3531 - val_loss: 0.4053\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3533 - val_loss: 0.3947\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3523 - val_loss: 0.3944\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3519 - val_loss: 0.3939\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3522 - val_loss: 0.3940\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3508 - val_loss: 0.3937\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3506 - val_loss: 0.3913\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3497 - val_loss: 0.3924\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3491 - val_loss: 0.3905\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3493 - val_loss: 0.3903\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3483 - val_loss: 0.3881\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3479 - val_loss: 0.3877\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3475 - val_loss: 0.3904\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3472 - val_loss: 0.3918\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3470 - val_loss: 0.3904\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3469 - val_loss: 0.3909\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3451 - val_loss: 0.3930\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3443 - val_loss: 0.3855\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3448 - val_loss: 0.3867\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3435 - val_loss: 0.3868\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3446 - val_loss: 0.3851\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3425 - val_loss: 0.3862\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3430 - val_loss: 0.3846\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3424 - val_loss: 0.3920\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3428 - val_loss: 0.3841\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3411 - val_loss: 0.3859\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3406 - val_loss: 0.3825\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3408 - val_loss: 0.3860\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3395 - val_loss: 0.3840\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3394 - val_loss: 0.3859\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3396 - val_loss: 0.3840\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3385 - val_loss: 0.3931\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3380 - val_loss: 0.3856\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3379 - val_loss: 0.3850\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3366 - val_loss: 0.3855\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3365 - val_loss: 0.3825\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3364 - val_loss: 0.3805\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3358 - val_loss: 0.3845\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3353 - val_loss: 0.3846\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3352 - val_loss: 0.3842\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3345 - val_loss: 0.3782\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3339 - val_loss: 0.3810\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3341 - val_loss: 0.3821\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3336 - val_loss: 0.3819\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3334 - val_loss: 0.3788\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3322 - val_loss: 0.3789\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3313 - val_loss: 0.3780\n",
      "121/121 [==============================] - 0s 430us/step - loss: 0.3650\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 8.0313 - val_loss: 6.0875\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 5.5575 - val_loss: 4.3525\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 4.0002 - val_loss: 3.2370\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 2.9806 - val_loss: 2.4975\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 2.3003 - val_loss: 1.9945\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 1.8326 - val_loss: 1.6446\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 1.5107 - val_loss: 1.3960\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 1.2835 - val_loss: 1.2176\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 1.1209 - val_loss: 1.0877\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.0030 - val_loss: 0.9923\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.9168 - val_loss: 0.9215\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.8533 - val_loss: 0.8687\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.8061 - val_loss: 0.8286\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7705 - val_loss: 0.7981\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7435 - val_loss: 0.7747\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.7228 - val_loss: 0.7563\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7067 - val_loss: 0.7420\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6941 - val_loss: 0.7303\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6839 - val_loss: 0.7208\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6755 - val_loss: 0.7130\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.6686 - val_loss: 0.7064\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6627 - val_loss: 0.7007\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6577 - val_loss: 0.6957\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6532 - val_loss: 0.6914\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6492 - val_loss: 0.6874\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6455 - val_loss: 0.6838\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6421 - val_loss: 0.6803\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6390 - val_loss: 0.6771\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6360 - val_loss: 0.6741\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6332 - val_loss: 0.6713\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6305 - val_loss: 0.6686\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6279 - val_loss: 0.6661\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6255 - val_loss: 0.6636\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6231 - val_loss: 0.6612\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6207 - val_loss: 0.6590\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6185 - val_loss: 0.6568\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6163 - val_loss: 0.6547\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6142 - val_loss: 0.6526\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6121 - val_loss: 0.6506\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6100 - val_loss: 0.6486\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6081 - val_loss: 0.6467\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6061 - val_loss: 0.6448\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6042 - val_loss: 0.6430\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6024 - val_loss: 0.6411\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6006 - val_loss: 0.6394\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5989 - val_loss: 0.6375\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5972 - val_loss: 0.6360\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5955 - val_loss: 0.6344\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5939 - val_loss: 0.6327\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5922 - val_loss: 0.6314\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5907 - val_loss: 0.6299\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5892 - val_loss: 0.6283\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5877 - val_loss: 0.6269\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 605us/step - loss: 0.5862 - val_loss: 0.6255\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.5848 - val_loss: 0.6242\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 615us/step - loss: 0.5834 - val_loss: 0.6227\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5821 - val_loss: 0.6214\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5807 - val_loss: 0.6200\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5795 - val_loss: 0.6188\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5782 - val_loss: 0.6175\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.5770 - val_loss: 0.6162\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5758 - val_loss: 0.6152\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 614us/step - loss: 0.5746 - val_loss: 0.6139\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.5734 - val_loss: 0.6129\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.5723 - val_loss: 0.6118\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.5712 - val_loss: 0.6108\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5701 - val_loss: 0.6098\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5690 - val_loss: 0.6086\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5680 - val_loss: 0.6075\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5670 - val_loss: 0.6065\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5660 - val_loss: 0.6054\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5650 - val_loss: 0.6045\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5641 - val_loss: 0.6038\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5632 - val_loss: 0.6028\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5623 - val_loss: 0.6019\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5614 - val_loss: 0.6009\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5606 - val_loss: 0.6001\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5597 - val_loss: 0.5993\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5589 - val_loss: 0.5984\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5581 - val_loss: 0.5976\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5573 - val_loss: 0.5971\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5565 - val_loss: 0.5962\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5558 - val_loss: 0.5955\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5551 - val_loss: 0.5950\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5543 - val_loss: 0.5940\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.5537 - val_loss: 0.5933\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5530 - val_loss: 0.5929\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5523 - val_loss: 0.5923\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5516 - val_loss: 0.5915\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5510 - val_loss: 0.5910\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5503 - val_loss: 0.5900\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5498 - val_loss: 0.5895\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5492 - val_loss: 0.5890\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5486 - val_loss: 0.5883\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5480 - val_loss: 0.5877\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5474 - val_loss: 0.5871\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5469 - val_loss: 0.5863\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5463 - val_loss: 0.5856\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5458 - val_loss: 0.5853\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5453 - val_loss: 0.5847\n",
      "121/121 [==============================] - 0s 429us/step - loss: 0.5516\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 7.8455 - val_loss: 6.1843\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 5.7984 - val_loss: 4.6457\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 4.3724 - val_loss: 3.5654\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 3.3638 - val_loss: 2.7956\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 2.6425 - val_loss: 2.2412\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 2.1214 - val_loss: 1.8375\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 1.7411 - val_loss: 1.5409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 1.4616 - val_loss: 1.3215\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.2549 - val_loss: 1.1583\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 1.1011 - val_loss: 1.0360\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.9861 - val_loss: 0.9440\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.8996 - val_loss: 0.8743\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.8342 - val_loss: 0.8212\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.7844 - val_loss: 0.7804\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7462 - val_loss: 0.7489\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7168 - val_loss: 0.7243\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6940 - val_loss: 0.7052\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6762 - val_loss: 0.6899\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6620 - val_loss: 0.6777\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6507 - val_loss: 0.6679\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6416 - val_loss: 0.6599\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6341 - val_loss: 0.6532\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6279 - val_loss: 0.6476\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6228 - val_loss: 0.6429\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6183 - val_loss: 0.6389\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.6145 - val_loss: 0.6353\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6112 - val_loss: 0.6321\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6082 - val_loss: 0.6293\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6055 - val_loss: 0.6267\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6031 - val_loss: 0.6244\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.6009 - val_loss: 0.6222\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5988 - val_loss: 0.6203\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5969 - val_loss: 0.6185\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5951 - val_loss: 0.6167\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5934 - val_loss: 0.6150\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5917 - val_loss: 0.6134\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5902 - val_loss: 0.6120\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5887 - val_loss: 0.6105\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5873 - val_loss: 0.6092\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5859 - val_loss: 0.6078\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5846 - val_loss: 0.6066\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5833 - val_loss: 0.6053\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5820 - val_loss: 0.6040\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5808 - val_loss: 0.6029\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5797 - val_loss: 0.6017\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5785 - val_loss: 0.6005\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5774 - val_loss: 0.5995\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5764 - val_loss: 0.5984\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5753 - val_loss: 0.5974\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5743 - val_loss: 0.5965\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5733 - val_loss: 0.5955\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5723 - val_loss: 0.5945\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5714 - val_loss: 0.5936\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5705 - val_loss: 0.5928\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5696 - val_loss: 0.5919\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5687 - val_loss: 0.5910\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5679 - val_loss: 0.5901\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5670 - val_loss: 0.5892\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5662 - val_loss: 0.5885\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5654 - val_loss: 0.5876\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5647 - val_loss: 0.5868\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5639 - val_loss: 0.5861\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5632 - val_loss: 0.5853\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5625 - val_loss: 0.5847\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5618 - val_loss: 0.5840\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5611 - val_loss: 0.5833\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5604 - val_loss: 0.5828\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5598 - val_loss: 0.5819\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5592 - val_loss: 0.5812\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5585 - val_loss: 0.5806\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.5579 - val_loss: 0.5799\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5573 - val_loss: 0.5793\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5568 - val_loss: 0.5789\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5562 - val_loss: 0.5782\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5557 - val_loss: 0.5776\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5552 - val_loss: 0.5770\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5546 - val_loss: 0.5765\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5541 - val_loss: 0.5760\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5536 - val_loss: 0.5755\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5531 - val_loss: 0.5750\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5527 - val_loss: 0.5747\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5522 - val_loss: 0.5741\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5518 - val_loss: 0.5737\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5513 - val_loss: 0.5734\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5509 - val_loss: 0.5728\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5505 - val_loss: 0.5724\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5501 - val_loss: 0.5721\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5497 - val_loss: 0.5718\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5493 - val_loss: 0.5712\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5489 - val_loss: 0.5709\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5485 - val_loss: 0.5703\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5482 - val_loss: 0.5699\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5478 - val_loss: 0.5696\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5475 - val_loss: 0.5692\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5471 - val_loss: 0.5688\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5468 - val_loss: 0.5684\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5465 - val_loss: 0.5679\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5461 - val_loss: 0.5675\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5459 - val_loss: 0.5673\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5455 - val_loss: 0.5669\n",
      "121/121 [==============================] - 0s 429us/step - loss: 0.5242\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 5.8247 - val_loss: 5.2206\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 4.3345 - val_loss: 3.9654\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 3.2971 - val_loss: 3.0807\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 2.5665 - val_loss: 2.4501\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 2.0467 - val_loss: 1.9963\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 1.6736 - val_loss: 1.6663\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 1.4038 - val_loss: 1.4247\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 1.2074 - val_loss: 1.2463\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 1.0637 - val_loss: 1.1138\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.9579 - val_loss: 1.0148\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.8797 - val_loss: 0.9403\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.8214 - val_loss: 0.8838\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.7778 - val_loss: 0.8406\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7449 - val_loss: 0.8074\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.7199 - val_loss: 0.7813\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.7006 - val_loss: 0.7609\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6857 - val_loss: 0.7445\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6739 - val_loss: 0.7314\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6645 - val_loss: 0.7206\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6568 - val_loss: 0.7116\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6504 - val_loss: 0.7040\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6450 - val_loss: 0.6975\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6404 - val_loss: 0.6917\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.6363 - val_loss: 0.6867\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6326 - val_loss: 0.6822\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6293 - val_loss: 0.6781\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.6263 - val_loss: 0.6743\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6234 - val_loss: 0.6709\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6208 - val_loss: 0.6676\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6182 - val_loss: 0.6646\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.6158 - val_loss: 0.6617\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6135 - val_loss: 0.6589\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6112 - val_loss: 0.6563\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.6090 - val_loss: 0.6538\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6069 - val_loss: 0.6513\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6048 - val_loss: 0.6489\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6028 - val_loss: 0.6466\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.6009 - val_loss: 0.6444\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5990 - val_loss: 0.6423\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5971 - val_loss: 0.6402\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5953 - val_loss: 0.6382\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5935 - val_loss: 0.6362\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5918 - val_loss: 0.6342\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5900 - val_loss: 0.6324\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5884 - val_loss: 0.6306\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5867 - val_loss: 0.6288\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5852 - val_loss: 0.6270\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5836 - val_loss: 0.6253\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.5821 - val_loss: 0.6237\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5806 - val_loss: 0.6221\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5791 - val_loss: 0.6205\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.5777 - val_loss: 0.6190\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5763 - val_loss: 0.6174\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5749 - val_loss: 0.6160\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.5735 - val_loss: 0.6146\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5722 - val_loss: 0.6132\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5709 - val_loss: 0.6118\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5697 - val_loss: 0.6104\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5684 - val_loss: 0.6091\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5672 - val_loss: 0.6078\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5660 - val_loss: 0.6065\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5649 - val_loss: 0.6053\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5637 - val_loss: 0.6041\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5626 - val_loss: 0.6029\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5615 - val_loss: 0.6018\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 614us/step - loss: 0.5605 - val_loss: 0.6006\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5594 - val_loss: 0.5996\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5584 - val_loss: 0.5985\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5574 - val_loss: 0.5974\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5564 - val_loss: 0.5964\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5554 - val_loss: 0.5954\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5545 - val_loss: 0.5944\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5536 - val_loss: 0.5934\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5527 - val_loss: 0.5925\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5518 - val_loss: 0.5915\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5509 - val_loss: 0.5906\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5501 - val_loss: 0.5897\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5493 - val_loss: 0.5888\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5484 - val_loss: 0.5880\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5477 - val_loss: 0.5872\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.5469 - val_loss: 0.5864\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5461 - val_loss: 0.5856\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5454 - val_loss: 0.5848\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5446 - val_loss: 0.5840\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5439 - val_loss: 0.5833\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5432 - val_loss: 0.5826\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.5425 - val_loss: 0.5819\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5419 - val_loss: 0.5812\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5412 - val_loss: 0.5805\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5406 - val_loss: 0.5798\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5399 - val_loss: 0.5791\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5393 - val_loss: 0.5785\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5387 - val_loss: 0.5778\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5381 - val_loss: 0.5772\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5376 - val_loss: 0.5766\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5370 - val_loss: 0.5760\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5364 - val_loss: 0.5754\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5359 - val_loss: 0.5749\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.5354 - val_loss: 0.5743\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5348 - val_loss: 0.5738\n",
      "121/121 [==============================] - 0s 397us/step - loss: 0.5567\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3026 - val_loss: 0.7162\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.7910 - val_loss: 0.6356\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6160 - val_loss: 0.5796\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5474 - val_loss: 0.5484\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5023 - val_loss: 0.5227\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4808 - val_loss: 0.5025\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.4673 - val_loss: 0.4914\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4534 - val_loss: 0.4790\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4456 - val_loss: 0.4840\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4381 - val_loss: 0.4661\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4315 - val_loss: 0.4587\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4241 - val_loss: 0.4513\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4197 - val_loss: 0.4460\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4144 - val_loss: 0.4455\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4102 - val_loss: 0.4389\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4099 - val_loss: 0.4547\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4056 - val_loss: 0.4332\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.4000 - val_loss: 0.4308\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3976 - val_loss: 0.4249\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3950 - val_loss: 0.4261\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3918 - val_loss: 0.4208\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3896 - val_loss: 0.4204\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.4146\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3833 - val_loss: 0.4228\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3865 - val_loss: 0.4115\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3804 - val_loss: 0.4112\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3764 - val_loss: 0.4077\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3780 - val_loss: 0.4048\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3736 - val_loss: 0.4024\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3713 - val_loss: 0.4032\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3687 - val_loss: 0.4000\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3670 - val_loss: 0.3978\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3656 - val_loss: 0.3993\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3631 - val_loss: 0.3966\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3617 - val_loss: 0.3959\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3610 - val_loss: 0.3913\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3579 - val_loss: 0.3942\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.3904\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.3940\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3548 - val_loss: 0.3874\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3527 - val_loss: 0.3852\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3521 - val_loss: 0.3866\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3500 - val_loss: 0.3833\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3494 - val_loss: 0.3822\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3464 - val_loss: 0.3812\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3478 - val_loss: 0.3799\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3491 - val_loss: 0.3797\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3428 - val_loss: 0.3771\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3413 - val_loss: 0.3782\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3410 - val_loss: 0.3764\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3395 - val_loss: 0.3746\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3378 - val_loss: 0.3731\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3419 - val_loss: 0.3756\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3384 - val_loss: 0.3704\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3340 - val_loss: 0.3868\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3787 - val_loss: 0.3724\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3330 - val_loss: 0.3687\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3344 - val_loss: 0.3746\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3302 - val_loss: 0.3667\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3305 - val_loss: 0.3669\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3303 - val_loss: 0.3654\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3269 - val_loss: 0.3645\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3258 - val_loss: 0.3623\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3252 - val_loss: 0.3626\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3245 - val_loss: 0.3675\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3266 - val_loss: 0.3691\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3222 - val_loss: 0.3778\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3233 - val_loss: 0.3634\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3253 - val_loss: 0.3632\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3217 - val_loss: 0.3640\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3214 - val_loss: 0.3580\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3269 - val_loss: 0.3589\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3172 - val_loss: 0.3588\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3155 - val_loss: 0.3578\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3145 - val_loss: 0.3547\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3152 - val_loss: 0.3551\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3136 - val_loss: 0.3544\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3129 - val_loss: 0.3538\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3121 - val_loss: 0.3558\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3119 - val_loss: 0.3580\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3111 - val_loss: 0.3654\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3691 - val_loss: 0.3562\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3102 - val_loss: 0.3564\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3098 - val_loss: 0.3540\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3083 - val_loss: 0.3524\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3075 - val_loss: 0.3512\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3067 - val_loss: 0.3491\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3058 - val_loss: 0.3523\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3054 - val_loss: 0.3502\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3044 - val_loss: 0.3493\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3048 - val_loss: 0.3474\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3033 - val_loss: 0.3484\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3014 - val_loss: 0.3483\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3011 - val_loss: 0.3466\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3065 - val_loss: 0.3461\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3024 - val_loss: 0.3471\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.2998 - val_loss: 0.3461\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2999 - val_loss: 0.3444\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2975 - val_loss: 0.3446\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.2983 - val_loss: 0.3430\n",
      "121/121 [==============================] - 0s 430us/step - loss: 0.3243\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2252 - val_loss: 0.7563\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6485 - val_loss: 0.6239\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5882 - val_loss: 0.5835\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5480 - val_loss: 0.5529\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5381 - val_loss: 0.5417\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5053 - val_loss: 0.5176\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4933 - val_loss: 0.5100\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4813 - val_loss: 0.4949\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4728 - val_loss: 0.5152\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4648 - val_loss: 0.5006\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4586 - val_loss: 0.4773\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4503 - val_loss: 0.4669\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4457 - val_loss: 0.4659\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4390 - val_loss: 0.4635\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4332 - val_loss: 0.4577\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4312 - val_loss: 0.4647\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4262 - val_loss: 0.4477\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4223 - val_loss: 0.4407\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4179 - val_loss: 0.4345\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4152 - val_loss: 0.4389\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4119 - val_loss: 0.4306\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4096 - val_loss: 0.4317\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4062 - val_loss: 0.4316\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4039 - val_loss: 0.4252\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4017 - val_loss: 0.4222\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4003 - val_loss: 0.4245\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3975 - val_loss: 0.4180\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3960 - val_loss: 0.4179\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3936 - val_loss: 0.4132\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3942 - val_loss: 0.4193\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3904 - val_loss: 0.4111\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3886 - val_loss: 0.4134\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3860 - val_loss: 0.4065\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3849 - val_loss: 0.4094\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3829 - val_loss: 0.4043\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3804 - val_loss: 0.4011\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3792 - val_loss: 0.4021\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3784 - val_loss: 0.3985\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3774 - val_loss: 0.3990\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3750 - val_loss: 0.3971\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3744 - val_loss: 0.3949\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3725 - val_loss: 0.3965\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3700 - val_loss: 0.3929\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3727 - val_loss: 0.3930\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3668 - val_loss: 0.3911\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3700 - val_loss: 0.3865\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3660 - val_loss: 0.3857\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3640 - val_loss: 0.3853\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3611 - val_loss: 0.3850\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3598 - val_loss: 0.3880\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3592 - val_loss: 0.3813\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3572 - val_loss: 0.3883\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3578 - val_loss: 0.3817\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3534 - val_loss: 0.3838\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3532 - val_loss: 0.3980\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3810 - val_loss: 0.3778\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3510 - val_loss: 0.3780\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3498 - val_loss: 0.3787\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3475 - val_loss: 0.3727\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3460 - val_loss: 0.3731\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.3448 - val_loss: 0.3723\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3442 - val_loss: 0.3700\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3419 - val_loss: 0.3709\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3423 - val_loss: 0.3679\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3409 - val_loss: 0.3732\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3401 - val_loss: 0.3711\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3389 - val_loss: 0.3701\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3377 - val_loss: 0.3670\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3380 - val_loss: 0.3650\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3358 - val_loss: 0.3649\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3347 - val_loss: 0.3617\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3338 - val_loss: 0.3641\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3339 - val_loss: 0.3718\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3331 - val_loss: 0.3609\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3308 - val_loss: 0.3634\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3302 - val_loss: 0.3574\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3291 - val_loss: 0.3581\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3296 - val_loss: 0.3580\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3276 - val_loss: 0.3559\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3264 - val_loss: 0.3564\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3254 - val_loss: 0.3621\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3253 - val_loss: 0.3520\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3236 - val_loss: 0.3533\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3229 - val_loss: 0.3549\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3222 - val_loss: 0.3516\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3220 - val_loss: 0.3561\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3217 - val_loss: 0.3600\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3208 - val_loss: 0.3513\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3191 - val_loss: 0.3493\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3187 - val_loss: 0.3524\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3195 - val_loss: 0.3512\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3182 - val_loss: 0.3510\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3173 - val_loss: 0.3501\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3163 - val_loss: 0.3434\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3149 - val_loss: 0.3459\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3143 - val_loss: 0.3432\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3140 - val_loss: 0.3431\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3146 - val_loss: 0.3428\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3118 - val_loss: 0.3417\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3112 - val_loss: 0.3419\n",
      "121/121 [==============================] - 0s 430us/step - loss: 0.3061\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1363 - val_loss: 0.7590\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.8285 - val_loss: 0.6837\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.6496 - val_loss: 0.6226\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5610 - val_loss: 0.5786\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5202 - val_loss: 0.5443\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4933 - val_loss: 0.5234\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4747 - val_loss: 0.5106\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4613 - val_loss: 0.4970\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4509 - val_loss: 0.4858\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4433 - val_loss: 0.4775\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4367 - val_loss: 0.4715\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4304 - val_loss: 0.4692\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4245 - val_loss: 0.4647\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4189 - val_loss: 0.4563\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4148 - val_loss: 0.4550\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4114 - val_loss: 0.4478\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4070 - val_loss: 0.4492\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4037 - val_loss: 0.4401\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4005 - val_loss: 0.4338\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3974 - val_loss: 0.4313\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3942 - val_loss: 0.4294\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3916 - val_loss: 0.4249\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3886 - val_loss: 0.4230\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3868 - val_loss: 0.4196\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3839 - val_loss: 0.4188\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3806 - val_loss: 0.4173\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3783 - val_loss: 0.4204\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3766 - val_loss: 0.4089\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3740 - val_loss: 0.4099\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3723 - val_loss: 0.4130\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3695 - val_loss: 0.4048\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3675 - val_loss: 0.4051\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3657 - val_loss: 0.4012\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3639 - val_loss: 0.4003\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3615 - val_loss: 0.3997\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3601 - val_loss: 0.3947\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3584 - val_loss: 0.3914\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3572 - val_loss: 0.3900\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3552 - val_loss: 0.3895\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3533 - val_loss: 0.3905\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3518 - val_loss: 0.3872\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3502 - val_loss: 0.3852\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3486 - val_loss: 0.3822\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3463 - val_loss: 0.3865\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3466 - val_loss: 0.3831\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3443 - val_loss: 0.3788\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.3431 - val_loss: 0.3799\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3413 - val_loss: 0.3753\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3404 - val_loss: 0.3754\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.3388 - val_loss: 0.3778\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3372 - val_loss: 0.3842\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3364 - val_loss: 0.3753\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3349 - val_loss: 0.3751\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3334 - val_loss: 0.3865\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3335 - val_loss: 0.3696\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3315 - val_loss: 0.3712\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3302 - val_loss: 0.3712\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3295 - val_loss: 0.3681\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3279 - val_loss: 0.3647\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3255 - val_loss: 0.3634\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3255 - val_loss: 0.3639\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3239 - val_loss: 0.3617\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3229 - val_loss: 0.3627\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3213 - val_loss: 0.3625\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3205 - val_loss: 0.3605\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3192 - val_loss: 0.3596\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3184 - val_loss: 0.3569\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3178 - val_loss: 0.3577\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3165 - val_loss: 0.3572\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3145 - val_loss: 0.3596\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3146 - val_loss: 0.3527\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3128 - val_loss: 0.3537\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3131 - val_loss: 0.3514\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3129 - val_loss: 0.3488\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3102 - val_loss: 0.3545\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3101 - val_loss: 0.3512\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3104 - val_loss: 0.3532\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3100 - val_loss: 0.3474\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3079 - val_loss: 0.3508\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3073 - val_loss: 0.3478\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3071 - val_loss: 0.3482\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3062 - val_loss: 0.3474\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3055 - val_loss: 0.3470\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3055 - val_loss: 0.3462\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3040 - val_loss: 0.3598\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3038 - val_loss: 0.3574\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3037 - val_loss: 0.3515\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3026 - val_loss: 0.3469\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3017 - val_loss: 0.3443\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3004 - val_loss: 0.3431\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3000 - val_loss: 0.3448\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2999 - val_loss: 0.3446\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2985 - val_loss: 0.3510\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2980 - val_loss: 0.3394\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2976 - val_loss: 0.3420\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2971 - val_loss: 0.3437\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2969 - val_loss: 0.3388\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2960 - val_loss: 0.3396\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2945 - val_loss: 0.3415\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2939 - val_loss: 0.3373\n",
      "121/121 [==============================] - 0s 429us/step - loss: 0.3363\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x0000028C682D2760>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-1185beffbb55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m rnd_search_cv.fit(X_train, y_train, epochs=100,\n\u001b[0m\u001b[0;32m     12\u001b[0m                  \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;31m# we clone again after setting params in case some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[0;32m    762\u001b[0m                 **self.best_params_))\n\u001b[0;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[0;32m     97\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                                (estimator, name))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x0000028C682D2760>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    'n_hidden' : [0, 1, 2, 3],\n",
    "    'n_neurons': np.arange(1,100),\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                 validation_data = (X_valid, y_valid),\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.32221129536628723"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnd_search_cv.best_estimator_\n",
    "# Throws an AttributeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
