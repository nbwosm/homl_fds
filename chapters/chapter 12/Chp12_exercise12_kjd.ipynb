{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HOML Ch12 Exercise 12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_tmJPrEz5bc"
      },
      "source": [
        "# HOML Chapter 12 Exercise 12\n",
        "\n",
        "## Exercise: Implement a custom layer that performs Layer Normalization (we will use this type of layer in Chapter 15):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBb8OXua0Q3C"
      },
      "source": [
        "*a. The build() method should define two trainable weights α and β, both of\n",
        "shape input_shape[-1:] and data type tf.float32. α should be initialized\n",
        "with 1s, and β with 0s.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooGZpqTpTRSa"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGtjztX3TRIR"
      },
      "source": [
        "# Random seeds from both Numpy and Tensorflow\n",
        "from numpy.random import seed\n",
        "seed(999)\n",
        "tf.random.set_seed(999)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huLJVHr80yWx"
      },
      "source": [
        "We'll set up two trainable weights - alpha and beta. In addition, we need a batch input shape because the number of units in the build method need to equal the number of inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljvzjXbz01Xi"
      },
      "source": [
        "# Build method\n",
        "def build(self, batch_input_shape):\n",
        "    self.alpha = self.add_weight(\n",
        "        name=\"alpha\", shape=batch_input_shape[-1:],\n",
        "        initializer=\"ones\")\n",
        "    self.beta = self.add_weight(\n",
        "        name=\"beta\", shape=batch_input_shape[-1:],\n",
        "        initializer=\"zeros\")\n",
        "    super().build(batch_input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qosL1OQk0RFF"
      },
      "source": [
        "*b. The call() method should compute the mean μ and standard deviation σ of\n",
        "each instance’s features. For this, you can use tf.nn.moments(inputs,\n",
        "axes=-1, keepdims=True), which returns the mean μ and the variance σ\n",
        "2 of\n",
        "all instances (compute the square root of the variance to get the standard\n",
        "deviation). Then the function should compute and return α⊗(X - μ)/(σ + ε) +\n",
        "β, where ⊗ represents itemwise multiplication (*) and ε is a smoothing term\n",
        "(small constant to avoid division by zero, e.g., 0.001).*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5SMqlKCatmW"
      },
      "source": [
        "We'll have to define the epsilon hyperparameter in the constructor. \n",
        "\n",
        "In the call method, we're going to include the epsilon value under the square root with the variance to ensure that we're never dividing by zero just in case the variance becomes zero. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv9x87b8P1fZ"
      },
      "source": [
        "# Custom Layer Normalization \n",
        "class LayerNormalization(keras.layers.Layer):\n",
        "    def __init__(self, epsilon=0.001, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        self.alpha = self.add_weight(\n",
        "            name=\"alpha\", shape=batch_input_shape[-1:],\n",
        "            initializer=\"ones\")\n",
        "        self.beta = self.add_weight(\n",
        "            name=\"beta\", shape=batch_input_shape[-1:],\n",
        "            initializer=\"zeros\")\n",
        "        super().build(batch_input_shape) # must be at the end\n",
        "\n",
        "    def call(self, X):\n",
        "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
        "        return self.alpha * (X - mean) / (tf.sqrt(variance + self.epsilon)) + self.beta\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        return batch_input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"epsilon\": self.epsilon}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcHhRfiu0RRT"
      },
      "source": [
        "*c. Ensure that your custom layer produces the same (or very nearly the same)\n",
        "output as the keras.layers.LayerNormalization layer.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAz5dkQ0Shd3"
      },
      "source": [
        "The author tested this custom layer on the California housing dataset. We'll do the same. Let's import it and split it into training, validation, and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UulP570S0b4"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okR_J8GHS5Gg",
        "outputId": "527a0756-79ef-4a1e-ffa7-f25673a06db7"
      },
      "source": [
        "housing = fetch_california_housing()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6qqleX6zsrH"
      },
      "source": [
        "X_train_all, X_test, y_train_all, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=999)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_all, y_train_all, random_state=999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PijJT1_rU6z7"
      },
      "source": [
        "Now, we need to convert the data to 32-bit float values for use in Tensorflow. To determine if both layer norms function similarly, we need to determine the mean of the difference in their mean absolute error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKRe9rKhTgZr"
      },
      "source": [
        "# Convert training values to float 32-bit\n",
        "X_train_32 = X_train.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhhl0jBMTzMy"
      },
      "source": [
        "# Define both the custom layer as well as Keras' LayerNormalization\n",
        "custom_ln = LayerNormalization()\n",
        "keras_ln = keras.layers.LayerNormalization()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUfoY02xT73b",
        "outputId": "285c4041-fd6b-48fc-b7cb-d85dedc216ad"
      },
      "source": [
        "# Find the mean of the mean abolute error between both layer norms\n",
        "tf.reduce_mean(keras.losses.mean_absolute_error(\n",
        "    keras_ln(X_train_32), custom_ln(X_train_32)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=3.7963805e-08>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M8-JBUzWkon"
      },
      "source": [
        "The difference between both layer norms is extremely small, so it appears that both layer norms work similarly.\n",
        "\n",
        "Just to be sure, the author decided to also test the difference between both layers by using randomly data. We'll do so as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YcNofiNUnx6"
      },
      "source": [
        "# Randomly generated data\n",
        "random_alpha = np.random.rand(X_train_32.shape[-1])\n",
        "random_beta = np.random.rand(X_train_32.shape[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5WbDWWtUpGe"
      },
      "source": [
        "# Set weights\n",
        "custom_ln.set_weights([random_alpha, random_beta])\n",
        "keras_ln.set_weights([random_alpha, random_beta])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwrxZrCMUFb_",
        "outputId": "4610c6ac-5379-486f-85ef-939403fff4a1"
      },
      "source": [
        "# Find the mean of the mean abolute error between both layer norms\n",
        "tf.reduce_mean(keras.losses.mean_absolute_error(\n",
        "    keras_ln(X_train_32), custom_ln(X_train_32)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=2.4424876e-08>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgDnl-9DaUZH"
      },
      "source": [
        "Again, the difference is negligibly small. The custom layer norm works as hoped."
      ]
    }
  ]
}