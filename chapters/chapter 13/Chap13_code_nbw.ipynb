{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow â‰¥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"data\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x <10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(7):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interleaving linges from multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To run this example need to load California housing dataset and split into multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak at few lines of csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\housing\\\\my_train_00.csv',\n",
       " 'datasets\\\\housing\\\\my_train_01.csv',\n",
       " 'datasets\\\\housing\\\\my_train_02.csv',\n",
       " 'datasets\\\\housing\\\\my_train_03.csv',\n",
       " 'datasets\\\\housing\\\\my_train_04.csv',\n",
       " 'datasets\\\\housing\\\\my_train_05.csv',\n",
       " 'datasets\\\\housing\\\\my_train_06.csv',\n",
       " 'datasets\\\\housing\\\\my_train_07.csv',\n",
       " 'datasets\\\\housing\\\\my_train_08.csv',\n",
       " 'datasets\\\\housing\\\\my_train_09.csv',\n",
       " 'datasets\\\\housing\\\\my_train_10.csv',\n",
       " 'datasets\\\\housing\\\\my_train_11.csv',\n",
       " 'datasets\\\\housing\\\\my_train_12.csv',\n",
       " 'datasets\\\\housing\\\\my_train_13.csv',\n",
       " 'datasets\\\\housing\\\\my_train_14.csv',\n",
       " 'datasets\\\\housing\\\\my_train_15.csv',\n",
       " 'datasets\\\\housing\\\\my_train_16.csv',\n",
       " 'datasets\\\\housing\\\\my_train_17.csv',\n",
       " 'datasets\\\\housing\\\\my_train_18.csv',\n",
       " 'datasets\\\\housing\\\\my_train_19.csv']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_13.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # header,\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\n",
      "b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\n",
      "b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n",
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.0>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'4'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Field 4 is interpreted as a string\n",
    "\n",
    "record_defaults=[0, np.nan, tf.constant(np.nan, dtype=tf.float64), \"Hello\", tf.constant([])]\n",
    "parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Hello'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing fields replaced with default value when provided\n",
    "\n",
    "parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# The 5th field is compulsory (since we provided tf.constant([]) as the \"default value\"), \n",
    "# so we get an exception if we do not provide it:\n",
    "\n",
    "try:\n",
    "    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# Number of fields should match exactly number of fields in record_defaults\n",
    "try:\n",
    "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8 # X_train.shape[-1]\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Dataset with tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n",
      "   1.2525111  -1.3671792 ]\n",
      " [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n",
      "   0.7231292  -1.0023477 ]\n",
      " [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n",
      "  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[1.752]\n",
      " [1.313]\n",
      " [1.535]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n",
      "   0.9807942  -0.67250353]\n",
      " [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n",
      "   1.107282   -0.8674123 ]\n",
      " [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n",
      "  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.919]\n",
      " [1.028]\n",
      " [2.182]], shape=(3, 1), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in train_set.take(2):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 2s 4ms/step - loss: 2.0914 - val_loss: 21.5124\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 0s 961us/step - loss: 0.8428 - val_loss: 0.6648\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 0s 947us/step - loss: 0.6329 - val_loss: 0.6196\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 0s 875us/step - loss: 0.5922 - val_loss: 0.5669\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 0s 837us/step - loss: 0.5622 - val_loss: 0.5402\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 0s 820us/step - loss: 0.5698 - val_loss: 0.5209\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 0s 828us/step - loss: 0.5195 - val_loss: 0.6130\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 0s 823us/step - loss: 0.5155 - val_loss: 0.4818\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 0s 825us/step - loss: 0.4965 - val_loss: 0.4904\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 0s 820us/step - loss: 0.4925 - val_loss: 0.4585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1927628ef70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
    "          validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 519us/step - loss: 0.4788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4787752032279968"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.3576407],\n",
       "       [2.255291 ],\n",
       "       [1.4437605],\n",
       "       ...,\n",
       "       [0.5654393],\n",
       "       [3.9442453],\n",
       "       [1.0232248]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set = test_set.map(lambda X, y: X)\n",
    "X_new = X_test\n",
    "model.predict(new_set, steps=len(X_new) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 1810/1810"
     ]
    }
   ],
   "source": [
    "# Create own training loop\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "total_steps = n_epochs * n_steps_per_epoch\n",
    "global_step = 0\n",
    "for X_batch, y_batch in train_set.take(total_steps):\n",
    "    global_step += 1\n",
    "    print(\"\\rGlobal step {}/{}\".format(global_step, total_steps), end=\"\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X_batch)\n",
    "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        loss = tf.add_n([main_loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF function that performs whole training loop\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32,\n",
    "          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
    "                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
    "                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 100 / 1810\n",
      "Global step 200 / 1810\n",
      "Global step 300 / 1810\n",
      "Global step 400 / 1810\n",
      "Global step 500 / 1810\n",
      "Global step 600 / 1810\n",
      "Global step 700 / 1810\n",
      "Global step 800 / 1810\n",
      "Global step 900 / 1810\n",
      "Global step 1000 / 1810\n",
      "Global step 1100 / 1810\n",
      "Global step 1200 / 1810\n",
      "Global step 1300 / 1810\n",
      "Global step 1400 / 1810\n",
      "Global step 1500 / 1810\n",
      "Global step 1600 / 1810\n",
      "Global step 1700 / 1810\n",
      "Global step 1800 / 1810\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size=32,\n",
    "          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
    "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
    "                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
    "                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
    "    n_steps_per_epoch = len(X_train) // batch_size\n",
    "    total_steps = n_epochs * n_steps_per_epoch\n",
    "    global_step = 0\n",
    "    for X_batch, y_batch in train_set.take(total_steps):\n",
    "        global_step += 1\n",
    "        if tf.equal(global_step % 100, 0):\n",
    "            tf.print(\"\\rGlobal step\", global_step, \"/\", total_steps)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* apply()              Applies a transformation function to this dataset.\n",
      "* as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n",
      "* batch()              Combines consecutive elements of this dataset into batches.\n",
      "* cache()              Caches the elements in this dataset.\n",
      "* cardinality()        Returns the cardinality of the dataset, if known.\n",
      "* concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      "* element_spec()       The type specification of an element of this dataset.\n",
      "* enumerate()          Enumerates the elements of this dataset.\n",
      "* filter()             Filters this dataset according to `predicate`.\n",
      "* flat_map()           Maps `map_func` across this dataset and flattens the result.\n",
      "* from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
      "* from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n",
      "* from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n",
      "* interleave()         Maps `map_func` across this dataset, and interleaves the results.\n",
      "* list_files()         A dataset of all files matching one or more glob patterns.\n",
      "* map()                Maps `map_func` across the elements of this dataset.\n",
      "* options()            Returns the options for this dataset and its inputs.\n",
      "* padded_batch()       Combines consecutive elements of this dataset into padded batches.\n",
      "* prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n",
      "* range()              Creates a `Dataset` of a step-separated range of values.\n",
      "* reduce()             Reduces the input dataset to a single element.\n",
      "* repeat()             Repeats this dataset so each original value is seen `count` times.\n",
      "* shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      "* shuffle()            Randomly shuffles the elements of this dataset.\n",
      "* skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n",
      "* take()               Creates a `Dataset` with at most `count` elements from this dataset.\n",
      "* unbatch()            Splits elements of a dataset into multiple elements.\n",
      "* window()             Combines (nests of) input elements into a dataset of (nests of) windows.\n",
      "* with_options()       Returns a new `tf.data.Dataset` with the given options set.\n",
      "* zip()                Creates a `Dataset` by zipping together the given datasets.\n"
     ]
    }
   ],
   "source": [
    "for m in dir(tf.data.Dataset):\n",
    "    if not (m.startswith(\"_\") or m.endswith(\"_\")):\n",
    "            func = getattr(tf.data.Dataset, m)\n",
    "            if hasattr(func, \"__doc__\"):\n",
    "                print(\"* {:21s}{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecord Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TFRecord file is just a list of binary records. You can create one using a tf.io.TFRecordWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read multiple TFRecord files with just one TFRecordDataset. By default it will read them one at a time, but if you set num_parallel_reads=3, it will read 3 at a time in parallel and interleave their records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'File 0 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 1 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 2 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 0 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 1 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 2 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 0 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 1 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 2 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 3 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 4 record 0', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 3 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 4 record 1', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 3 record 2', shape=(), dtype=string)\n",
      "tf.Tensor(b'File 4 record 2', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_test_{}.tfrecord\".format(i) for i in range(5)]\n",
    "for i, filepath in enumerate(filepaths):\n",
    "    with tf.io.TFRecordWriter(filepath) as f:\n",
    "        for j in range(3):\n",
    "            f.write(\"File {} record {}\".format(i, j).encode(\"utf-8\"))\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
    "                                  compression_type=\"GZIP\")\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief Introduction to Protocol Buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did not run this section due to issues with protoc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile person.proto\n",
    "# syntax = \"proto3\";\n",
    "# message Person {\n",
    "#   string name = 1;\n",
    "#   int32 id = 2;\n",
    "#   repeated string email = 3;\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls person*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from person_pb2 import Person\n",
    "\n",
    "# person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\n",
    "# print(person)  # display the Person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book code not contained in github repo for first part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features using one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(housing['ocean_proximity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(housing['ocean_proximity'].unique())\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 5, 2, 3, 2], dtype=int64)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'NEAR OCEAN', 'INLAND'])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.6645621 , 0.44100678],\n",
       "       [0.3528825 , 0.46448255],\n",
       "       [0.03366041, 0.68467236],\n",
       "       [0.74011743, 0.8724445 ],\n",
       "       [0.22632635, 0.22319686],\n",
       "       [0.3103881 , 0.7223358 ],\n",
       "       [0.13318717, 0.5480639 ]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 5, 2, 3, 2], dtype=int64)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'NEAR OCEAN', 'INLAND'])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[0.6645621 , 0.44100678],\n",
       "       [0.3103881 , 0.7223358 ],\n",
       "       [0.03366041, 0.68467236],\n",
       "       [0.74011743, 0.8724445 ],\n",
       "       [0.03366041, 0.68467236]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = keras.layers.Embedding(input_dim=len(vocab)+num_oov_buckets, output_dim = embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[ 0.01878912, -0.00155212],\n",
       "       [-0.02383961,  0.01973433],\n",
       "       [ 0.0231154 ,  0.03925682],\n",
       "       [ 0.04467482,  0.02493341],\n",
       "       [ 0.0231154 ,  0.03925682]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_inputs = keras.layers.Input(shape=[8])\n",
    "categories = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
    "cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
    "model = keras.models.Model(inputs=[regular_inputs, categories], outputs = [outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Processing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book version only contains summary code. Github repo doesn't contain anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow_transform as tft\n",
    "\n",
    "    def preprocess(inputs):  # inputs is a batch of input features\n",
    "        median_age = inputs[\"housing_median_age\"]\n",
    "        ocean_proximity = inputs[\"ocean_proximity\"]\n",
    "        standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\n",
    "        ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "        return {\n",
    "            \"standardized_median_age\": standardized_age,\n",
    "            \"ocean_proximity_id\": ocean_proximity_id\n",
    "        }\n",
    "except ImportError:\n",
    "    print(\"TF Transform is not installed. Try running: pip3 install -U tensorflow-transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset', 'ai2_arc', 'ai2_arc_with_ir', 'amazon_us_reviews', 'anli', 'arc', 'bair_robot_pushing_small', 'bccd', 'beans', 'big_patent', 'bigearthnet', 'billsum', 'binarized_mnist', 'binary_alpha_digits', 'blimp', 'bool_q', 'c4', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cars196', 'cassava', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'cfq', 'cherry_blossoms', 'chexpert', 'cifar10', 'cifar100', 'cifar10_1', 'cifar10_corrupted', 'citrus_leaves', 'cityscapes', 'civil_comments', 'clevr', 'clic', 'clinc_oos', 'cmaterdb', 'cnn_dailymail', 'coco', 'coco_captions', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'common_voice', 'coqa', 'cos_e', 'cosmos_qa', 'covid19sum', 'crema_d', 'curated_breast_imaging_ddsm', 'cycle_gan', 'd4rl_mujoco_ant', 'd4rl_mujoco_halfcheetah', 'dart', 'davis', 'deep_weeds', 'definite_pronoun_resolution', 'dementiabank', 'diabetic_retinopathy_detection', 'div2k', 'dmlab', 'dolphin_number_word', 'downsampled_imagenet', 'drop', 'dsprites', 'dtd', 'duke_ultrasound', 'e2e_cleaned', 'efron_morris75', 'emnist', 'eraser_multi_rc', 'esnli', 'eurosat', 'fashion_mnist', 'flic', 'flores', 'food101', 'forest_fires', 'fuss', 'gap', 'geirhos_conflict_stimuli', 'gem', 'genomics_ood', 'german_credit_numeric', 'gigaword', 'glue', 'goemotions', 'gpt3', 'gref', 'groove', 'gtzan', 'gtzan_music_speech', 'hellaswag', 'higgs', 'horses_or_humans', 'howell', 'i_naturalist2017', 'imagenet2012', 'imagenet2012_corrupted', 'imagenet2012_real', 'imagenet2012_subset', 'imagenet_a', 'imagenet_r', 'imagenet_resized', 'imagenet_v2', 'imagenette', 'imagewang', 'imdb_reviews', 'irc_disentanglement', 'iris', 'kitti', 'kmnist', 'lambada', 'lfw', 'librispeech', 'librispeech_lm', 'libritts', 'ljspeech', 'lm1b', 'lost_and_found', 'lsun', 'lvis', 'malaria', 'math_dataset', 'mctaco', 'mlqa', 'mnist', 'mnist_corrupted', 'movie_lens', 'movie_rationales', 'movielens', 'moving_mnist', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'natural_questions', 'natural_questions_open', 'newsroom', 'nsynth', 'nyu_depth_v2', 'ogbg_molpcba', 'omniglot', 'open_images_challenge2019_detection', 'open_images_v4', 'openbookqa', 'opinion_abstracts', 'opinosis', 'opus', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'patch_camelyon', 'paws_wiki', 'paws_x_wiki', 'pet_finder', 'pg19', 'piqa', 'places365_small', 'plant_leaves', 'plant_village', 'plantae_k', 'qa4mre', 'qasc', 'quac', 'quickdraw_bitmap', 'race', 'radon', 'reddit', 'reddit_disentanglement', 'reddit_tifu', 'resisc45', 'robonet', 'rock_paper_scissors', 'rock_you', 's3o4d', 'salient_span_wikipedia', 'samsum', 'savee', 'scan', 'scene_parse150', 'schema_guided_dialogue', 'scicite', 'scientific_papers', 'sentiment140', 'shapes3d', 'siscore', 'smallnorb', 'snli', 'so2sat', 'speech_commands', 'spoken_digit', 'squad', 'stanford_dogs', 'stanford_online_products', 'star_cfq', 'starcraft_video', 'stl10', 'story_cloze', 'sun397', 'super_glue', 'svhn_cropped', 'tao', 'ted_hrlr_translate', 'ted_multi_translate', 'tedlium', 'tf_flowers', 'the300w_lp', 'tiny_shakespeare', 'titanic', 'trec', 'trivia_qa', 'tydi_qa', 'uc_merced', 'ucf101', 'vctk', 'vgg_face2', 'visual_domain_decathlon', 'voc', 'voxceleb', 'voxforge', 'waymo_open_dataset', 'web_nlg', 'web_questions', 'wider_face', 'wiki40b', 'wiki_bio', 'wiki_table_questions', 'wiki_table_text', 'wikiann', 'wikihow', 'wikipedia', 'wikipedia_toxicity_subtypes', 'wine_quality', 'winogrande', 'wmt13_translate', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'wordnet', 'wsc273', 'xnli', 'xquad', 'xsum', 'xtreme_pawsx', 'xtreme_xnli', 'yelp_polarity_reviews', 'yes_no', 'youtube_vis']\n"
     ]
    }
   ],
   "source": [
    "print(tfds.list_builders())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "mnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\n",
    "for item in mnist_train:\n",
    "    images = item[\"image\"]\n",
    "    labels = item[\"label\"]\n",
    "    for index in range(5):\n",
    "        plt.subplot(1, 5, index + 1)\n",
    "        image = images[index, ..., 0]\n",
    "        label = labels[index].numpy()\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.title(label)\n",
    "        plt.axis(\"off\")\n",
    "    break # just showing part of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1)\n",
      "[4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1 0 6 6 9 9 4 8 9 4 7 3 3]\n"
     ]
    }
   ],
   "source": [
    "datasets = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
    "mnist_train = mnist_train.repeat(5).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)\n",
    "for images, labels in mnist_train.take(1):\n",
    "    print(images.shape)\n",
    "    print(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 698us/step - loss: 42.9108 - accuracy: 0.8034\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 485us/step - loss: 24.8643 - accuracy: 0.8701\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 481us/step - loss: 24.3896 - accuracy: 0.8718\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 479us/step - loss: 23.5890 - accuracy: 0.8775\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 532us/step - loss: 22.9626 - accuracy: 0.8798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19283482dc0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
    "mnist_train = datasets[\"train\"].repeat().prefetch(1)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Lambda(lambda images: tf.cast(images, tf.float32)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
