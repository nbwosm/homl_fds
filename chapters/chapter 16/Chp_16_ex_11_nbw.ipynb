{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16 Exercise 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import shutil\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFOpenAIGPTLMHeadModel.\n",
      "\n",
      "All the layers of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel\n",
    "\n",
    "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187]])>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text,\n",
    "                                  add_special_tokens=False,\n",
    "                                  return_tensors=\"tf\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 50), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   636,   868,   580,  9818,   485,  5018,   715,   239,\n",
       "          244, 40477,  1473,  1699,   239, 40477,   500,   481,  5332,\n",
       "          240,    26,   518,  4616,  6212,   481, 10022,  1081,   562,\n",
       "          481,  1541,   239,   524,  2600,   509,  1241,   557,  7816,\n",
       "          557,   669,   487,   656,   694],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   500,   481,  9458,   260, 16841,  5883,   498,   481,\n",
       "         1926,    26, 27690,   239,   725,  2283,   815,   481,  6404,\n",
       "          240,   557,   544,   481,  7613,   240,   544,   481,  1866,\n",
       "          498,   524,  8140,   240,   618,    10,   837,   500,   239,\n",
       "         2034,   481,  1866,   498,   618],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   239,   481,  3445,   498,  2060, 24183,   240,   481,\n",
       "         2595,   498, 26794,   488,  6617,   240,   481,  7672,   563,\n",
       "        18271, 27491,   522,  2433,  1392,  7976,   649,   246,  6093,\n",
       "          500,   246,  4417,   844,   498,   246,  2278,   260,  1218,\n",
       "         1122,   239,  4187,   240,   487],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   636,  2150,   246,  4206,   485,   606,   248,   504,\n",
       "          239,   249,  1048,  2874,   485,  1767,   240,   249,   812,\n",
       "          595,  1229,   504,   616,  3573,   485,   799,   485,  3234,\n",
       "        12223,   240,   244,   487,   603,   500,   531,  1241,  2700,\n",
       "         1002,   239, 40477,   487,  1241],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   498,   567,   767,  2310,   240,   244,   487,  4985,\n",
       "          240,   244,   488,   481, 15213,   488, 15213,   498,   481,\n",
       "         1807,   500,   984,   507,  2763,   240,   500,   984,   481,\n",
       "          618,   600,   641,   488,   589,  1175,   240,   759,   580,\n",
       "          749, 11051,  1339,   239,   244]])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle would never be theirs to rule over. \" \n",
      " everyone laughed. \n",
      " in the palace, llorel wandered the halls looking for the boy. his company was almost as soothing as when he 'd been\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle in the pre - reign period of the pallbearers. more important than the throne, as is the title, is the kiss of his majesty, king dwalin. upon the kiss of king\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle. the whisper of walking carpets, the smell of gilt and tile, the occasional unflattering digit or single word uttered like a groan in a government room of a hundred - years old. gods, he\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle would become a castle to we d on. i am scared to death, i will not stand on this island to go to seagard, \" he said in an almost normal voice. \n",
      " he almost\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle of mastodon, \" he wrote, \" and the courts and courts of the city in which it lies, in which the king they were and all men, can be... harmonious. \"\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle. \" \n",
      " the king's eyes narrowed and he looked at me with a look that was almost as cold as his voice when i told him about my dream in which we had been separated by water and\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle is the home to a great many people. it has been said that king arthur was once one who ruled over all lands and peoples in his time ; but now he must be replaced by another ruler,\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle is the most important place in all our lands. it's where we're going to be staying for a while and i'm sure you 'll want your rest as well before that happens.'\n",
      " sparhawk\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle is the home to a great many people. it was once called'king's palace'because its walls were made from gold and silver that had been mined by men in ancient times ; but now they\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle is the most important place in all our lands. \" \n",
      " he paused and looked at his son with a look that was almost sad as if it were an old friend who had died rather than be here\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=0.2,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=3.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `TFBertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "All model checkpoint layers were used when initializing TFBertLMHeadModel.\n",
      "\n",
      "All the layers of TFBertLMHeadModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertLMHeadModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertLMHeadModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author links to transformers run_generation.py page, but that is no longer extant. Find it here:\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-generation/run_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11), dtype=int32, numpy=\n",
       "array([[ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842]])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text,\n",
    "                                  add_special_tokens=False,\n",
    "                                  return_tensors=\"tf\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_prompt = encoded_prompt.numpy()\n",
    "np_prompt = np_prompt[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 51), dtype=int32, numpy=\n",
       "array([[ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842,  2829,  1010,  1010,  1010,  1010,  1998,  1998,\n",
       "         1998,  1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,\n",
       "         1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,\n",
       "         1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,\n",
       "         1012,  2133,  2133,  2090,  1997,  1998],\n",
       "       [ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842,  2035,  2035,  2035,  2035,  2035,  2035,  2053,\n",
       "         2053,  2053,  2053,  2053,  2035,  2035,  2035,  2035,  2035,\n",
       "         2035,  2035,  1998,  1998,  1998,  1998,  1998,  1998,  1998,\n",
       "         1998,  1998,  1998,  1998,  1998,  1998,  1998,  1998,  1998,\n",
       "         1998,  1998,  1998,  1998,  1998,  1998],\n",
       "       [ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842,  2521,  2521,  2521,  2521,  2053,  2053,  2053,\n",
       "         2053,  2053,  2053,   999,   999,   999,   999,   999,   999,\n",
       "          999,   999,   999,   999,   999,   999,   999,   999,   999,\n",
       "          999,   999,   999,   999,   999,   999,   999,   999,   999,\n",
       "          999,   999,   999,   999,   999,   999],\n",
       "       [ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842,  2061,  2061,  2061,  2061,  2061,  2061,  2061,\n",
       "         2061,  2061,  2061,  2061,  2061,  2061,  2061,  2061,  2061,\n",
       "         2061,  2061,  2061,  2061,  2061,  2061,  2061,  2061,  2061,\n",
       "         2061,  2061,  2061,  2061,  2061,  2061,  2061,  2061,  2061,\n",
       "         2061,  2061,  2061,  2061,  2061,  2061],\n",
       "       [ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842,  2035,  2035,  2035,  2053,  2035,  2035,  2035,\n",
       "         2035,  2035,  2035,  2035,  2035,  2035,  2035,  1996,  1996,\n",
       "         2079,  2079,  2079,  2079,  2000,  1998,  1998,  1998,  1998,\n",
       "         1998,  1998,  1998,  1998,  1998,  1998,  1998,  1998,  1998,\n",
       "         1998,  1998,  1998,  1998,  1998,  1998]])>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle brown,,,, and and and................................. between of and\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle all all all all all all no no no no no all all all all all all all and and and and and and and and and and and and and and and and and and and and and and\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle far far far far no no no no no no!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle all all all no all all all all all all all all all all the the do do do do to and and and and and and and and and and and and and and and and and and and\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 51), dtype=int32, numpy=\n",
       "array([[ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842,  2042,  2054,  2001,  2020,  2025,  5053,  2009,\n",
       "         2035,  1996,  1998,  2029,  2062,  2021,  2008,  2055,  2102,\n",
       "         2002,  2045,  2016,  2383,  2044,  2013,  2032,  2058,  2010,\n",
       "         2029,  2000,  2061,  2122,  2028,  2894,  1999,  1037,  2002,\n",
       "         2173,  2178,  2005,  2005,  2004,  2253],\n",
       "       [ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842,  5192,  7804, 11418,  6925,  2466,  2409,  8867,\n",
       "         7122,  2030,  1998,  2823,  1011,  2774,  2195,  1000,  2005,\n",
       "         2001,  2000, 11937,  4830,  2014,  2129,  2021,  2061,  2106,\n",
       "         2070,  1005, 15536,  1051,  2035,  2045,  2205,  2008,  2083,\n",
       "         2908,  2091,  2735,  3308,  2357,  4332],\n",
       "       [ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842, 23384,  2485,  2958,  2379,  1012,  1029,  2133,\n",
       "         2821,  2748,  2092,  7910,  9413,  9686,  1005,  8038, 13442,\n",
       "        29349,  2108,  2632,  2056,  5287,  2182,  2000,  2039,  3647,\n",
       "         1999,  2041,  2035,  1998,  2021,  2292,  2008,  2022,  2003,\n",
       "         2009,  2054,  2518,  2064,  2045,  2059],\n",
       "       [ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842,  2101,  2969,  3238,  1998,  2062,  2625,  2084,\n",
       "         2013,  2108,  2025,  2066,  3773,  2156,  2079,  2033,  2041,\n",
       "         2009,  2035,  5199,  2122,  1037,  3170,  8110,  2001,  1005,\n",
       "         2062,  1049,  2030,  2070,  2116,  2174,  1999,  2172,  2021,\n",
       "         2746,  2145,  1996, 14804,  2879,  1996],\n",
       "       [ 2023,  2548,  6106,  1997,  5465,  1010,  2023,  8040, 23606,\n",
       "         5596,  8842,  4125,  2145,  1012,  1029,  1011,   999,  1028,\n",
       "         1026, 25627, 16226, 12312, 16416,  2951,  2451,  6925,  7122,\n",
       "        10628, 12909, 12143, 29556, 27364,  3075, 25725,  8025,  3669,\n",
       "         3085,  2319,  2588,  2004,  2107,  2021,  2664,  2085,  1998,\n",
       "         2061,  2116,  1999,  2092,  2129,  2035]])>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=5.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle been what was were not beauty it all the and which more but that aboutt he there she having after from him over his which to so these one alone in a he place another for for as went\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle shadow goddess rim tale story told fairy tales or and sometimes - songs several \" for was to ta da her how but so did some'wi o all there too that through gone down turn wrong turned turns\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle ness close bridge near.?... oh yes well uh er es'ya aye nay being al said spoken here to up safe in out all and but let that be is it what thing can there then\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle later selfless and more less than from being not like seeing see do me out it all tim these aine filling was'more m or some many however in much but coming still the lad boy the\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle rise still.? -! > < cobalt motif sagarea data community tale tales curiosity luna speculation boredomdot libraryross curiousliablean upon as such but yet now and so many in well how all\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle island all over and or it something there just out for me myself like as such not no yes is still now so if i my mine you other others some more many yet gone away long far lost loss\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle so not no it all over there yeah well ok yes oh ah still some bit fun lit out off and or as a an '. s d c t r & / etc... em -! #\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle island all over and or it something out in for the me him his my no so i he still is will not do would could we us be our some a s many other okay yeah wells they\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle island all like about just be me more some advice wisdom wise lost far many or so and out forth yet not still always well is it she her'a s aye nay but no do to how what\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle so not no it all nothing everything well is be still and that let for out some the will last long yet but oh ah aye yes yeah right left gone now away looking up over back again me okay\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=0.2,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=3.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
