{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 15: Exercise 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/jsb_chorales.tgz\n",
      "122880/117661 [===============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\n",
    "FILENAME = \"jsb_chorales.tgz\"\n",
    "filepath = keras.utils.get_file(FILENAME,\n",
    "                                DOWNLOAD_ROOT + FILENAME,\n",
    "                                cache_subdir=\"datasets/jsb_chorales\",\n",
    "                                extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsb_chorales_dir = Path(filepath).parent\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [75, 70, 58, 55],\n",
       " [75, 70, 58, 55],\n",
       " [75, 70, 60, 55],\n",
       " [75, 70, 60, 55],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 70, 62, 55],\n",
       " [77, 70, 62, 55],\n",
       " [77, 69, 62, 55],\n",
       " [77, 69, 62, 55],\n",
       " [75, 67, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [75, 69, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [77, 65, 62, 50],\n",
       " [77, 65, 62, 50],\n",
       " [77, 65, 60, 50],\n",
       " [77, 65, 60, 50],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 53],\n",
       " [74, 67, 58, 53],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [75, 72, 55, 48],\n",
       " [75, 72, 55, 48],\n",
       " [75, 72, 55, 50],\n",
       " [75, 72, 55, 50],\n",
       " [75, 67, 60, 51],\n",
       " [75, 67, 60, 51],\n",
       " [75, 67, 60, 53],\n",
       " [75, 67, 60, 53],\n",
       " [74, 67, 60, 55],\n",
       " [74, 67, 60, 55],\n",
       " [74, 67, 57, 55],\n",
       " [74, 67, 57, 55],\n",
       " [74, 65, 59, 43],\n",
       " [74, 65, 59, 43],\n",
       " [72, 63, 59, 43],\n",
       " [72, 63, 59, 43],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [77, 70, 62, 58],\n",
       " [77, 70, 62, 58],\n",
       " [77, 70, 62, 56],\n",
       " [77, 70, 62, 56],\n",
       " [79, 70, 62, 55],\n",
       " [79, 70, 62, 55],\n",
       " [79, 70, 62, 53],\n",
       " [79, 70, 62, 53],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [77, 70, 63, 58],\n",
       " [77, 70, 63, 58],\n",
       " [77, 70, 60, 58],\n",
       " [77, 70, 60, 58],\n",
       " [77, 70, 62, 46],\n",
       " [77, 70, 62, 46],\n",
       " [77, 68, 62, 46],\n",
       " [75, 68, 62, 46],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [75, 67, 58, 53],\n",
       " [75, 67, 58, 53],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [77, 65, 58, 50],\n",
       " [77, 65, 58, 50],\n",
       " [77, 65, 56, 50],\n",
       " [77, 65, 56, 50],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [75, 67, 58, 57],\n",
       " [75, 67, 58, 57],\n",
       " [75, 67, 58, 55],\n",
       " [75, 67, 58, 55],\n",
       " [77, 65, 60, 57],\n",
       " [77, 65, 60, 57],\n",
       " [77, 65, 60, 53],\n",
       " [77, 65, 60, 53],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chorales[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36\n",
    "assert max_note == 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "def notes_to_frequencies(notes):\n",
    "    # Frequency doubles when you go up one octave; there are 12 semi-tones\n",
    "    # per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n",
    "    return 2 ** ((np.array(notes) - 69) / 12) * 440\n",
    "\n",
    "def frequencies_to_samples(frequencies, tempo, sample_rate):\n",
    "    note_duration = 60 / tempo # the tempo is measured in beats per minutes\n",
    "    # To reduce click sound at every beat, we round the frequencies to try to\n",
    "    # get the samples close to zero at the end of each note.\n",
    "    frequencies = np.round(note_duration * frequencies) / note_duration\n",
    "    n_samples = int(note_duration * sample_rate)\n",
    "    time = np.linspace(0, note_duration, n_samples)\n",
    "    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n",
    "    # Removing all notes with frequencies ≤ 9 Hz (includes note 0 = silence)\n",
    "    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n",
    "    return sine_waves.reshape(-1)\n",
    "\n",
    "def chords_to_samples(chords, tempo, sample_rate):\n",
    "    freqs = notes_to_frequencies(chords)\n",
    "    freqs = np.r_[freqs, freqs[-1:]] # make last note a bit longer\n",
    "    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n",
    "                     for melody in freqs.T], axis=0)\n",
    "    n_fade_out_samples = sample_rate * 60 // tempo # fade out last note\n",
    "    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n",
    "    merged[-n_fade_out_samples:] *= fade_out\n",
    "    return merged\n",
    "\n",
    "def play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n",
    "    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n",
    "    if filepath:\n",
    "        from scipy.io import wavfile\n",
    "        samples = (2**15 * samples).astype(np.int16)\n",
    "        wavfile.write(filepath, sample_rate, samples)\n",
    "        return display(Audio(filepath))\n",
    "    else:\n",
    "        return display(Audio(samples, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(3):\n",
    "    play_chords(train_chorales[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(batch):\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n",
    "    return tf.reshape(window, [-1]) # convert to arpegio\n",
    "\n",
    "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                 window_size=32, window_shift=16, cache=True):\n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        return dataset.flat_map(batch_window)\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(create_target)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = bach_dataset(train_chorales, shuffle_buffer_size=1000)\n",
    "valid_set = bach_dataset(valid_chorales)\n",
    "test_set = bach_dataset(test_chorales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 5)           235       \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 32)          352       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 48)          3120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 48)          192       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 96)          12384     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 96)          384       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 256)         361472    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 47)          12079     \n",
      "=================================================================\n",
      "Total params: 396,810\n",
      "Trainable params: 396,330\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_embedding_dims = 5\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.LSTM(256, return_sequences=True),\n",
    "    keras.layers.Dense(n_notes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "98/98 [==============================] - 25s 222ms/step - loss: 2.6569 - accuracy: 0.3377 - val_loss: 3.7108 - val_accuracy: 0.0702\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 21s 214ms/step - loss: 0.9572 - accuracy: 0.7539 - val_loss: 3.7097 - val_accuracy: 0.0777\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 20s 208ms/step - loss: 0.7725 - accuracy: 0.7887 - val_loss: 3.6994 - val_accuracy: 0.1388\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 21s 210ms/step - loss: 0.7063 - accuracy: 0.8006 - val_loss: 2.5132 - val_accuracy: 0.3260\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 23s 232ms/step - loss: 0.6398 - accuracy: 0.8153 - val_loss: 1.8150 - val_accuracy: 0.4615\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 23s 236ms/step - loss: 0.6011 - accuracy: 0.8243 - val_loss: 1.3084 - val_accuracy: 0.5928\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 22s 226ms/step - loss: 0.5688 - accuracy: 0.8314 - val_loss: 0.7699 - val_accuracy: 0.7749\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 24s 242ms/step - loss: 0.5290 - accuracy: 0.8413 - val_loss: 0.6335 - val_accuracy: 0.8151\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 25s 256ms/step - loss: 0.4987 - accuracy: 0.8496 - val_loss: 0.6316 - val_accuracy: 0.8162\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 22s 226ms/step - loss: 0.4766 - accuracy: 0.8542 - val_loss: 0.6048 - val_accuracy: 0.8241\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 22s 222ms/step - loss: 0.4495 - accuracy: 0.8625 - val_loss: 0.6295 - val_accuracy: 0.8171\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 22s 227ms/step - loss: 0.4251 - accuracy: 0.8677 - val_loss: 0.6594 - val_accuracy: 0.8104\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 22s 229ms/step - loss: 0.4057 - accuracy: 0.8742 - val_loss: 0.6398 - val_accuracy: 0.8151\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 25s 251ms/step - loss: 0.3816 - accuracy: 0.8817 - val_loss: 0.6774 - val_accuracy: 0.8048\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 23s 233ms/step - loss: 0.3736 - accuracy: 0.8836 - val_loss: 0.6259 - val_accuracy: 0.8181\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 23s 238ms/step - loss: 0.3469 - accuracy: 0.8917 - val_loss: 0.6219 - val_accuracy: 0.8233\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 22s 227ms/step - loss: 0.3257 - accuracy: 0.8986 - val_loss: 0.6106 - val_accuracy: 0.8253\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 24s 242ms/step - loss: 0.3076 - accuracy: 0.9044 - val_loss: 0.6419 - val_accuracy: 0.8150\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 23s 230ms/step - loss: 0.2980 - accuracy: 0.9080 - val_loss: 0.6379 - val_accuracy: 0.8194\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 22s 223ms/step - loss: 0.2793 - accuracy: 0.9136 - val_loss: 0.6474 - val_accuracy: 0.8156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25eb8146640>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=20, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chorale(model, seed_chords, length):\n",
    "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
    "    arpegio = tf.reshape(arpegio, [1, -1])\n",
    "    for chord in range(length):\n",
    "        for note in range(4):\n",
    "            #next_note = model.predict_classes(arpegio)[:1, -1:]\n",
    "            next_note = np.argmax(model.predict(arpegio), axis=-1)[:1, -1:]\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    return tf.reshape(arpegio, shape=[-1, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_chords = test_chorales[2][:8]\n",
    "play_chords(seed_chords, amplitude=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chorale = generate_chorale(model, seed_chords, 56)\n",
    "play_chords(new_chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chorale_v2(model, seed_chords, length, temperature=1):\n",
    "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
    "    arpegio = tf.reshape(arpegio, [1, -1])\n",
    "    for chord in range(length):\n",
    "        for note in range(4):\n",
    "            next_note_probas = model.predict(arpegio)[0, -1:]\n",
    "            rescaled_logits = tf.math.log(next_note_probas) / temperature\n",
    "            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    return tf.reshape(arpegio, shape=[-1, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chorale_v2_cold = generate_chorale_v2(model, seed_chords, 56, temperature=0.8)\n",
    "play_chords(new_chorale_v2_cold, filepath=\"bach_cold.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chorale_v2_medium = generate_chorale_v2(model, seed_chords, 56, temperature=1.0)\n",
    "play_chords(new_chorale_v2_medium, filepath=\"bach_medium.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chorale_v2_hot = generate_chorale_v2(model, seed_chords, 56, temperature=1.5)\n",
    "play_chords(new_chorale_v2_hot, filepath=\"bach_hot.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_chords(test_chorales[2][:64], filepath=\"bach_test_4.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
