{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Ch7_exercise8_kjd.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVexLZBUF0Uy"
      },
      "source": [
        "# HOML Chapter 7 - Ensemble Learning and Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHL3Xe4aF0Vb"
      },
      "source": [
        "## Exercise 8\n",
        "\n",
        "Load the MNIST data (introduced in Chapter 3), and split it into a training set, a\n",
        "validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). \n",
        "\n",
        "Then train various classifiers, such as a Random\n",
        "Forest classifier, an Extra-Trees classifier, and an SVM. \n",
        "\n",
        "Next, try to combine\n",
        "them into an ensemble that outperforms them all on the validation set, using a\n",
        "soft or hard voting classifier. \n",
        "\n",
        "Once you have found one, try it on the test set. How\n",
        "much better does it perform compared to the individual classifiers?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUp8H25LF0Vh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQzRS4CPF0V9"
      },
      "source": [
        "Let's start by loading the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npxX6ORdF0WK"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "mnist.target = mnist.target.astype(np.uint8)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2exDGf1CF0Wn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI4ITv7VF0XP"
      },
      "source": [
        "# Split the original dataset into training/validation and testing. Use 10000 for the test set as sugggested by the author. \n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    mnist.data, mnist.target, test_size=10000, random_state=999)\n",
        "\n",
        "# Then split the training set into training/validation set into separate training and validation sets. Use 10000 for the validation set as sugggested by the author.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=10000, random_state=999)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X2FrXXwF0Xf"
      },
      "source": [
        "Now that we've split the data, let's train it on several classifiers. The author suggests random forest, extra trees, and SVM. We'll also add AdaBoost since it's covered in this chapter. However, we'll just stick with its default estimator, DecisionTreeClassifier, to add more variety to our classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQLzmOpQF0Xh"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B95Ua9BoF0Xt"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, random_state=999)\n",
        "et = ExtraTreesClassifier(n_estimators=100, random_state=999)\n",
        "ada = AdaBoostClassifier(n_estimators=100, random_state=999)\n",
        "svm = SVC(random_state=999, probability=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8bYAJ-rF0X5",
        "outputId": "8ef08033-5b43-4f59-f980-578b44d15c8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print the selected and default hyperparameters for each classifier and fit them to the training data. Note that this process will take several minutes to complete. \n",
        "classifiers = [rf, et, ada, svm]\n",
        "for clf in classifiers:\n",
        "    print(clf)\n",
        "    clf.fit(X_train, y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=999,\n",
            "                       verbose=0, warm_start=False)\n",
            "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
            "                     criterion='gini', max_depth=None, max_features='auto',\n",
            "                     max_leaf_nodes=None, max_samples=None,\n",
            "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                     min_samples_leaf=1, min_samples_split=2,\n",
            "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                     n_jobs=None, oob_score=False, random_state=999, verbose=0,\n",
            "                     warm_start=False)\n",
            "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
            "                   n_estimators=100, random_state=999)\n",
            "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=True, random_state=999, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwGxEd-NJFOf",
        "outputId": "72e589fe-a0ba-48ff-a9bf-3aed384e173a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# List of scores on the validation set for each classifier\n",
        "[clf.score(X_val, y_val) for clf in classifiers]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9675, 0.9717, 0.7154, 0.9781]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tqI-g-UHTkw"
      },
      "source": [
        "The AdaBoost classifier performed far more poorly than the other classifiers. We didn't change its base estimator from DecsisonTreeClassifier, which may be why it performed as it did. We'll eliminate it and use VotingClassifier to get an ensemble score with the remaining three classifiers. We'll start with hard voting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIYJtMREF0Y3"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIepwbrlF0ZF"
      },
      "source": [
        "# List of classifers to be used for VotingClassifier\n",
        "estimators = [('random forest', rf), ('extra trees', et), ('svm', svm)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_riS9ohuF0ZV"
      },
      "source": [
        "voting = VotingClassifier(estimators)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEpeHW1XF0Zh",
        "outputId": "c9b993b6-b1d9-4ed0-f3ff-b25d137095cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Fit the training set with the voting classifier. As with the fitting step above, this will take several minutes to complete. \n",
        "voting.fit(X_train, y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('random forest',\n",
              "                              RandomForestClassifier(bootstrap=True,\n",
              "                                                     ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     criterion='gini',\n",
              "                                                     max_depth=None,\n",
              "                                                     max_features='auto',\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     max_samples=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_impurity_split=None,\n",
              "                                                     min_samples_leaf=1,\n",
              "                                                     min_samples_split=2,\n",
              "                                                     min_weight_fraction_leaf=0.0,\n",
              "                                                     n_estimators=100,\n",
              "                                                     n_jobs=None...\n",
              "                                                   n_jobs=None, oob_score=False,\n",
              "                                                   random_state=999, verbose=0,\n",
              "                                                   warm_start=False)),\n",
              "                             ('svm',\n",
              "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                                  class_weight=None, coef0=0.0,\n",
              "                                  decision_function_shape='ovr', degree=3,\n",
              "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                                  probability=True, random_state=999,\n",
              "                                  shrinking=True, tol=0.001, verbose=False))],\n",
              "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
              "                 weights=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edMswjF5F0Zr",
        "outputId": "b8ad2e08-5b3e-4fad-af73-466c43135725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Validation score for hard voting \n",
        "voting.score(X_val, y_val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9733"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPgUXSI8-MBG"
      },
      "source": [
        "The hard voting ensemble did fairly well, but it didn't outperform the SVM classifer. Let's try soft voting and see if it can outperform all of the individual classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p29D1CvTF0aK"
      },
      "source": [
        "# Change to soft voting\n",
        "voting.voting = \"soft\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP14rMPNF0aS",
        "outputId": "3ba3512f-475b-45e4-8b39-a6c2496d4e44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Validation score for soft voting\n",
        "voting.score(X_val, y_val)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9787"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYwm_W9p-rr4"
      },
      "source": [
        "The soft voting ensemble just beat out the SVM classifier, so let's now run it on the test set and see if it can outperform each of the individual classifiers there, too. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0WESucOlwdy",
        "outputId": "b1df768b-c72a-42d0-815e-f040d820faac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test score for soft voting\n",
        "voting.score(X_test, y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9783"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0Upf_47mEnw",
        "outputId": "b9ca524b-f6a7-47c7-9540-0dfeb933ba00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# List of test scores for each classifier in soft voting ensemble\n",
        "[clf.score(X_test, y_test) for clf in voting.estimators_]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.967, 0.9716, 0.9776]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BduD5FGJ_hrB"
      },
      "source": [
        "Again, the soft voting ensemble outperforms all of its individual classifiers."
      ]
    }
  ]
}